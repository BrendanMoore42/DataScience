{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## <center> Missing Value Introduction\n",
    "Source: https://www.kaggle.com/dansbecker/handling-missing-values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-14T01:24:35.559612Z",
     "start_time": "2018-02-14T01:24:34.855243Z"
    },
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "main_file_path = 'Data/train.csv'\n",
    "data = pd.read_csv(main_file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-14T01:25:51.167044Z",
     "start_time": "2018-02-14T01:25:51.164696Z"
    },
    "collapsed": false,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "#print(data.isnull().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### Null Value Solutions\n",
    "##### 1) A Simple Option: Drop Columns with Missing Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "original_data = pd.DataFrame(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>MSSubClass</th>\n",
       "      <th>MSZoning</th>\n",
       "      <th>LotArea</th>\n",
       "      <th>Street</th>\n",
       "      <th>LotShape</th>\n",
       "      <th>LandContour</th>\n",
       "      <th>Utilities</th>\n",
       "      <th>LotConfig</th>\n",
       "      <th>LandSlope</th>\n",
       "      <th>...</th>\n",
       "      <th>EnclosedPorch</th>\n",
       "      <th>3SsnPorch</th>\n",
       "      <th>ScreenPorch</th>\n",
       "      <th>PoolArea</th>\n",
       "      <th>MiscVal</th>\n",
       "      <th>MoSold</th>\n",
       "      <th>YrSold</th>\n",
       "      <th>SaleType</th>\n",
       "      <th>SaleCondition</th>\n",
       "      <th>SalePrice</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>60</td>\n",
       "      <td>RL</td>\n",
       "      <td>8450</td>\n",
       "      <td>Pave</td>\n",
       "      <td>Reg</td>\n",
       "      <td>Lvl</td>\n",
       "      <td>AllPub</td>\n",
       "      <td>Inside</td>\n",
       "      <td>Gtl</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>2008</td>\n",
       "      <td>WD</td>\n",
       "      <td>Normal</td>\n",
       "      <td>208500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>20</td>\n",
       "      <td>RL</td>\n",
       "      <td>9600</td>\n",
       "      <td>Pave</td>\n",
       "      <td>Reg</td>\n",
       "      <td>Lvl</td>\n",
       "      <td>AllPub</td>\n",
       "      <td>FR2</td>\n",
       "      <td>Gtl</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>2007</td>\n",
       "      <td>WD</td>\n",
       "      <td>Normal</td>\n",
       "      <td>181500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>60</td>\n",
       "      <td>RL</td>\n",
       "      <td>11250</td>\n",
       "      <td>Pave</td>\n",
       "      <td>IR1</td>\n",
       "      <td>Lvl</td>\n",
       "      <td>AllPub</td>\n",
       "      <td>Inside</td>\n",
       "      <td>Gtl</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>9</td>\n",
       "      <td>2008</td>\n",
       "      <td>WD</td>\n",
       "      <td>Normal</td>\n",
       "      <td>223500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>70</td>\n",
       "      <td>RL</td>\n",
       "      <td>9550</td>\n",
       "      <td>Pave</td>\n",
       "      <td>IR1</td>\n",
       "      <td>Lvl</td>\n",
       "      <td>AllPub</td>\n",
       "      <td>Corner</td>\n",
       "      <td>Gtl</td>\n",
       "      <td>...</td>\n",
       "      <td>272</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>2006</td>\n",
       "      <td>WD</td>\n",
       "      <td>Abnorml</td>\n",
       "      <td>140000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>60</td>\n",
       "      <td>RL</td>\n",
       "      <td>14260</td>\n",
       "      <td>Pave</td>\n",
       "      <td>IR1</td>\n",
       "      <td>Lvl</td>\n",
       "      <td>AllPub</td>\n",
       "      <td>FR2</td>\n",
       "      <td>Gtl</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>12</td>\n",
       "      <td>2008</td>\n",
       "      <td>WD</td>\n",
       "      <td>Normal</td>\n",
       "      <td>250000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 62 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Id  MSSubClass MSZoning  LotArea Street LotShape LandContour Utilities  \\\n",
       "0   1          60       RL     8450   Pave      Reg         Lvl    AllPub   \n",
       "1   2          20       RL     9600   Pave      Reg         Lvl    AllPub   \n",
       "2   3          60       RL    11250   Pave      IR1         Lvl    AllPub   \n",
       "3   4          70       RL     9550   Pave      IR1         Lvl    AllPub   \n",
       "4   5          60       RL    14260   Pave      IR1         Lvl    AllPub   \n",
       "\n",
       "  LotConfig LandSlope    ...    EnclosedPorch 3SsnPorch ScreenPorch PoolArea  \\\n",
       "0    Inside       Gtl    ...                0         0           0        0   \n",
       "1       FR2       Gtl    ...                0         0           0        0   \n",
       "2    Inside       Gtl    ...                0         0           0        0   \n",
       "3    Corner       Gtl    ...              272         0           0        0   \n",
       "4       FR2       Gtl    ...                0         0           0        0   \n",
       "\n",
       "  MiscVal  MoSold  YrSold  SaleType  SaleCondition SalePrice  \n",
       "0       0       2    2008        WD         Normal    208500  \n",
       "1       0       5    2007        WD         Normal    181500  \n",
       "2       0       9    2008        WD         Normal    223500  \n",
       "3       0       2    2006        WD        Abnorml    140000  \n",
       "4       0      12    2008        WD         Normal    250000  \n",
       "\n",
       "[5 rows x 62 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_without_missing_values = original_data.dropna(axis=1)\n",
    "data_without_missing_values.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "##### 2) A Better Option: Imputation\n",
    "* Imputation fills in the missing value with some number. The imputed value won't be exactly right in most cases, but it usually gives more accurate models than dropping the column entirely.\n",
    "* The default behavior fills in the mean value for imputation. Statisticians have researched more complex strategies, but those complex strategies typically give no benefit once you plug the results into sophisticated machine learning models.\n",
    "* One (of many) nice things about Imputation is that it can be included in a scikit-learn Pipeline. Pipelines simplify model building, model validation and model deployment.\n",
    "\n",
    "* Warning: Sklearn imputation only work on numeric values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1460 entries, 0 to 1459\n",
      "Data columns (total 81 columns):\n",
      "Id               1460 non-null int64\n",
      "MSSubClass       1460 non-null int64\n",
      "MSZoning         1460 non-null object\n",
      "LotFrontage      1201 non-null float64\n",
      "LotArea          1460 non-null int64\n",
      "Street           1460 non-null object\n",
      "Alley            91 non-null object\n",
      "LotShape         1460 non-null object\n",
      "LandContour      1460 non-null object\n",
      "Utilities        1460 non-null object\n",
      "LotConfig        1460 non-null object\n",
      "LandSlope        1460 non-null object\n",
      "Neighborhood     1460 non-null object\n",
      "Condition1       1460 non-null object\n",
      "Condition2       1460 non-null object\n",
      "BldgType         1460 non-null object\n",
      "HouseStyle       1460 non-null object\n",
      "OverallQual      1460 non-null int64\n",
      "OverallCond      1460 non-null int64\n",
      "YearBuilt        1460 non-null int64\n",
      "YearRemodAdd     1460 non-null int64\n",
      "RoofStyle        1460 non-null object\n",
      "RoofMatl         1460 non-null object\n",
      "Exterior1st      1460 non-null object\n",
      "Exterior2nd      1460 non-null object\n",
      "MasVnrType       1452 non-null object\n",
      "MasVnrArea       1452 non-null float64\n",
      "ExterQual        1460 non-null object\n",
      "ExterCond        1460 non-null object\n",
      "Foundation       1460 non-null object\n",
      "BsmtQual         1423 non-null object\n",
      "BsmtCond         1423 non-null object\n",
      "BsmtExposure     1422 non-null object\n",
      "BsmtFinType1     1423 non-null object\n",
      "BsmtFinSF1       1460 non-null int64\n",
      "BsmtFinType2     1422 non-null object\n",
      "BsmtFinSF2       1460 non-null int64\n",
      "BsmtUnfSF        1460 non-null int64\n",
      "TotalBsmtSF      1460 non-null int64\n",
      "Heating          1460 non-null object\n",
      "HeatingQC        1460 non-null object\n",
      "CentralAir       1460 non-null object\n",
      "Electrical       1459 non-null object\n",
      "1stFlrSF         1460 non-null int64\n",
      "2ndFlrSF         1460 non-null int64\n",
      "LowQualFinSF     1460 non-null int64\n",
      "GrLivArea        1460 non-null int64\n",
      "BsmtFullBath     1460 non-null int64\n",
      "BsmtHalfBath     1460 non-null int64\n",
      "FullBath         1460 non-null int64\n",
      "HalfBath         1460 non-null int64\n",
      "BedroomAbvGr     1460 non-null int64\n",
      "KitchenAbvGr     1460 non-null int64\n",
      "KitchenQual      1460 non-null object\n",
      "TotRmsAbvGrd     1460 non-null int64\n",
      "Functional       1460 non-null object\n",
      "Fireplaces       1460 non-null int64\n",
      "FireplaceQu      770 non-null object\n",
      "GarageType       1379 non-null object\n",
      "GarageYrBlt      1379 non-null float64\n",
      "GarageFinish     1379 non-null object\n",
      "GarageCars       1460 non-null int64\n",
      "GarageArea       1460 non-null int64\n",
      "GarageQual       1379 non-null object\n",
      "GarageCond       1379 non-null object\n",
      "PavedDrive       1460 non-null object\n",
      "WoodDeckSF       1460 non-null int64\n",
      "OpenPorchSF      1460 non-null int64\n",
      "EnclosedPorch    1460 non-null int64\n",
      "3SsnPorch        1460 non-null int64\n",
      "ScreenPorch      1460 non-null int64\n",
      "PoolArea         1460 non-null int64\n",
      "PoolQC           7 non-null object\n",
      "Fence            281 non-null object\n",
      "MiscFeature      54 non-null object\n",
      "MiscVal          1460 non-null int64\n",
      "MoSold           1460 non-null int64\n",
      "YrSold           1460 non-null int64\n",
      "SaleType         1460 non-null object\n",
      "SaleCondition    1460 non-null object\n",
      "SalePrice        1460 non-null int64\n",
      "dtypes: float64(3), int64(35), object(43)\n",
      "memory usage: 924.0+ KB\n"
     ]
    }
   ],
   "source": [
    "original_data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# isolating numeric values\n",
    "numeric_columns  = list(original_data.select_dtypes(include=[int, float]).columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  1.00000000e+00,   6.00000000e+01,   6.50000000e+01, ...,\n",
       "          2.00000000e+00,   2.00800000e+03,   2.08500000e+05],\n",
       "       [  2.00000000e+00,   2.00000000e+01,   8.00000000e+01, ...,\n",
       "          5.00000000e+00,   2.00700000e+03,   1.81500000e+05],\n",
       "       [  3.00000000e+00,   6.00000000e+01,   6.80000000e+01, ...,\n",
       "          9.00000000e+00,   2.00800000e+03,   2.23500000e+05],\n",
       "       ..., \n",
       "       [  1.45800000e+03,   7.00000000e+01,   6.60000000e+01, ...,\n",
       "          5.00000000e+00,   2.01000000e+03,   2.66500000e+05],\n",
       "       [  1.45900000e+03,   2.00000000e+01,   6.80000000e+01, ...,\n",
       "          4.00000000e+00,   2.01000000e+03,   1.42125000e+05],\n",
       "       [  1.46000000e+03,   2.00000000e+01,   7.50000000e+01, ...,\n",
       "          6.00000000e+00,   2.00800000e+03,   1.47500000e+05]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.preprocessing import Imputer\n",
    "my_imputer =  Imputer()\n",
    "data_with_imputed_values = my_imputer.fit_transform(original_data[numeric_columns])\n",
    "data_with_imputed_values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "##### 3) An Extention to Imputation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# make a copy to avoid changing original data (when Imputing)\n",
    "new_data = original_data[numeric_columns].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# make new columns indicating what will be imputed\n",
    "cols_with_missing = (col for col in new_data.columns\n",
    "                    if new_data[col].isnull().any())\n",
    "\n",
    "for col in cols_with_missing:\n",
    "  new_data[col + '_was missing'] = new_data[col].isnull()\n",
    "  \n",
    "# Imputation\n",
    "my_imputer = Imputer()\n",
    "new_data = my_imputer.fit_transform(new_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### Example - Comparing All Null Value Solution\n",
    "* Question: What does out-of-sample MAE score mean?\n",
    "##### Basic Problem Set-up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "# Load data\n",
    "melb_data = pd.read_csv('Data/melb_data.csv')\n",
    "\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "melb_target = melb_data.Price\n",
    "melb_predictors = melb_data.drop(['Price'], axis=1)\n",
    "\n",
    "# For the sake of keeping the example simple, we'll use only numeric predictors.\n",
    "melb_numeric_predictors = melb_predictors.select_dtypes(exclude=['object'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "##### Creating Function to Measure Quality of An Approach\n",
    "Question: What the fuck does Random State =0 mean?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(melb_numeric_predictors, \n",
    "                                                    melb_target,\n",
    "                                                    train_size=0.7, \n",
    "                                                    test_size=0.3, \n",
    "                                                    random_state=0)\n",
    "\n",
    "def score_dataset(X_train, X_test, y_train, y_test):\n",
    "  model = RandomForestRegressor()\n",
    "  model.fit(X_train, y_train)\n",
    "  preds = model.predict(X_test)\n",
    "  return (mean_absolute_error(y_test, preds))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "##### Get Model Score from Dropping Columns with Missing Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false,
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Absolute Error form dropping columsn with Missing Values:\n",
      "350680.848922\n"
     ]
    }
   ],
   "source": [
    "cols_with_missing = [col for col in X_train.columns \n",
    "                     if X_train[col].isnull().any()]\n",
    "                     \n",
    "reduced_X_train = X_train.drop(cols_with_missing, axis=1)\n",
    "reduced_X_test = X_test.drop(cols_with_missing, axis=1)\n",
    "print('Mean Absolute Error form dropping columsn with Missing Values:')\n",
    "print(score_dataset(reduced_X_train, reduced_X_test, y_train, y_test))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "hidden": true
   },
   "source": [
    "##### Get Model Score from imputation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false,
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Absolute Error from Impuation:\n",
      "202939.546548\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import Imputer\n",
    "\n",
    "my_impture = Imputer()\n",
    "imputed_X_train = my_imputer.fit_transform(X_train)\n",
    "imputed_X_test = my_imputer.transform(X_test)\n",
    "print('Mean Absolute Error from Impuation:')\n",
    "print(score_dataset(imputed_X_train, imputed_X_test, y_train, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "##### Get Score from Imputation with Extra Columns Showing What Was Imputed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false,
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Absolute Error from Imputation while Track What Was Imputed:\n",
      "201093.903261\n"
     ]
    }
   ],
   "source": [
    "imputed_X_train_plus = X_train.copy()\n",
    "imputed_X_test_plus = X_test.copy()\n",
    "\n",
    "cols_with_missing =  (col for col in X_train.columns\n",
    "                     if X_train[col].isnull().any())\n",
    "\n",
    "for col in cols_with_missing:\n",
    "  imputed_X_train_plus[col + '_was_missing'] = imputed_X_train_plus[col].isnull()\n",
    "  imputed_X_test_plus[col + '_was_missing'] = imputed_X_test_plus[col].isnull()\n",
    "  \n",
    "  \n",
    "# Imputation\n",
    "my_imputer = Imputer()\n",
    "imputed_X_train_plus = my_imputer.fit_transform(imputed_X_train_plus)\n",
    "imputed_X_test_plus = my_imputer.fit_transform(imputed_X_test_plus)\n",
    "\n",
    "print(\"Mean Absolute Error from Imputation while Track What Was Imputed:\")\n",
    "print(score_dataset(imputed_X_train_plus, imputed_X_test_plus, y_train, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "* In this case, the extension didn't make a big difference. As mentioned before, this can vary widely from one dataset to the next (largely determined by whether rows with missing values are intrinsically like or unlike those without missing values)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### Using Categorical Data with One Hot Encoding\n",
    "Source: https://www.kaggle.com/dansbecker/using-categorical-data-with-one-hot-encoding\n",
    "\n",
    "Question: Why use One Hot Encoding over Label Encoding?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Setup Code\n",
    "\n",
    "# Read the data\n",
    "import pandas as pd\n",
    "train_data = pd.read_csv('Data/train.csv')\n",
    "test_data = pd.read_csv('Data/test.csv')\n",
    "\n",
    "# Drop the houses where the target is missing\n",
    "train_data.dropna(axis=0, subset=['SalePrice'], inplace=True)\n",
    "\n",
    "target = train_data.SalePrice\n",
    "\n",
    "# Author Notes:\n",
    "## Since missing values isn't the focus of this tutorial, we use the simplest\n",
    "## possible approach, which drops these columns. \n",
    "## For more detail (and a better approach) to missing values, see\n",
    "## https://www.kaggle.com/dansbecker/handling-missing-values\n",
    "\n",
    "\n",
    "cols_with_missing = [col for col in train_data.columns\n",
    "                    if train_data[col].isnull().any()]\n",
    "\n",
    "\n",
    "candidate_train_predictors = train_data.drop(['Id', 'SalePrice'] + cols_with_missing, axis=1)\n",
    "candidate_test_predictors = test_data.drop(['Id'] + cols_with_missing, axis=1)\n",
    "\n",
    "\n",
    "low_cardinality_cols = [cname for cname in candidate_train_predictors.columns if \n",
    "                                candidate_train_predictors[cname].nunique() < 10 and\n",
    "                                candidate_train_predictors[cname].dtype == \"object\"]\n",
    "numeric_cols = [cname for cname in candidate_train_predictors.columns if \n",
    "                                candidate_train_predictors[cname].dtype in ['int64', 'float64']]\n",
    "my_cols = low_cardinality_cols + numeric_cols\n",
    "train_predictors = candidate_train_predictors[my_cols]\n",
    "test_predictors = candidate_test_predictors[my_cols]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "* \"cardinality\" means the number of unique values in a column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false,
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ScreenPorch       int64\n",
       "LotArea           int64\n",
       "OverallQual       int64\n",
       "BldgType         object\n",
       "YrSold            int64\n",
       "YearRemodAdd      int64\n",
       "LotShape         object\n",
       "MiscVal           int64\n",
       "EnclosedPorch     int64\n",
       "Foundation       object\n",
       "dtype: object"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_predictors.dtypes.sample(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false,
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>MSSubClass</th>\n",
       "      <th>LotArea</th>\n",
       "      <th>OverallQual</th>\n",
       "      <th>OverallCond</th>\n",
       "      <th>YearBuilt</th>\n",
       "      <th>YearRemodAdd</th>\n",
       "      <th>BsmtFinSF1</th>\n",
       "      <th>BsmtFinSF2</th>\n",
       "      <th>BsmtUnfSF</th>\n",
       "      <th>TotalBsmtSF</th>\n",
       "      <th>...</th>\n",
       "      <th>SaleType_ConLw</th>\n",
       "      <th>SaleType_New</th>\n",
       "      <th>SaleType_Oth</th>\n",
       "      <th>SaleType_WD</th>\n",
       "      <th>SaleCondition_Abnorml</th>\n",
       "      <th>SaleCondition_AdjLand</th>\n",
       "      <th>SaleCondition_Alloca</th>\n",
       "      <th>SaleCondition_Family</th>\n",
       "      <th>SaleCondition_Normal</th>\n",
       "      <th>SaleCondition_Partial</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>60</td>\n",
       "      <td>8450</td>\n",
       "      <td>7</td>\n",
       "      <td>5</td>\n",
       "      <td>2003</td>\n",
       "      <td>2003</td>\n",
       "      <td>706</td>\n",
       "      <td>0</td>\n",
       "      <td>150</td>\n",
       "      <td>856</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>20</td>\n",
       "      <td>9600</td>\n",
       "      <td>6</td>\n",
       "      <td>8</td>\n",
       "      <td>1976</td>\n",
       "      <td>1976</td>\n",
       "      <td>978</td>\n",
       "      <td>0</td>\n",
       "      <td>284</td>\n",
       "      <td>1262</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>60</td>\n",
       "      <td>11250</td>\n",
       "      <td>7</td>\n",
       "      <td>5</td>\n",
       "      <td>2001</td>\n",
       "      <td>2002</td>\n",
       "      <td>486</td>\n",
       "      <td>0</td>\n",
       "      <td>434</td>\n",
       "      <td>920</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>70</td>\n",
       "      <td>9550</td>\n",
       "      <td>7</td>\n",
       "      <td>5</td>\n",
       "      <td>1915</td>\n",
       "      <td>1970</td>\n",
       "      <td>216</td>\n",
       "      <td>0</td>\n",
       "      <td>540</td>\n",
       "      <td>756</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>60</td>\n",
       "      <td>14260</td>\n",
       "      <td>8</td>\n",
       "      <td>5</td>\n",
       "      <td>2000</td>\n",
       "      <td>2000</td>\n",
       "      <td>655</td>\n",
       "      <td>0</td>\n",
       "      <td>490</td>\n",
       "      <td>1145</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>50</td>\n",
       "      <td>14115</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>1993</td>\n",
       "      <td>1995</td>\n",
       "      <td>732</td>\n",
       "      <td>0</td>\n",
       "      <td>64</td>\n",
       "      <td>796</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>20</td>\n",
       "      <td>10084</td>\n",
       "      <td>8</td>\n",
       "      <td>5</td>\n",
       "      <td>2004</td>\n",
       "      <td>2005</td>\n",
       "      <td>1369</td>\n",
       "      <td>0</td>\n",
       "      <td>317</td>\n",
       "      <td>1686</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>60</td>\n",
       "      <td>10382</td>\n",
       "      <td>7</td>\n",
       "      <td>6</td>\n",
       "      <td>1973</td>\n",
       "      <td>1973</td>\n",
       "      <td>859</td>\n",
       "      <td>32</td>\n",
       "      <td>216</td>\n",
       "      <td>1107</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>50</td>\n",
       "      <td>6120</td>\n",
       "      <td>7</td>\n",
       "      <td>5</td>\n",
       "      <td>1931</td>\n",
       "      <td>1950</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>952</td>\n",
       "      <td>952</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>190</td>\n",
       "      <td>7420</td>\n",
       "      <td>5</td>\n",
       "      <td>6</td>\n",
       "      <td>1939</td>\n",
       "      <td>1950</td>\n",
       "      <td>851</td>\n",
       "      <td>0</td>\n",
       "      <td>140</td>\n",
       "      <td>991</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>20</td>\n",
       "      <td>11200</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>1965</td>\n",
       "      <td>1965</td>\n",
       "      <td>906</td>\n",
       "      <td>0</td>\n",
       "      <td>134</td>\n",
       "      <td>1040</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>60</td>\n",
       "      <td>11924</td>\n",
       "      <td>9</td>\n",
       "      <td>5</td>\n",
       "      <td>2005</td>\n",
       "      <td>2006</td>\n",
       "      <td>998</td>\n",
       "      <td>0</td>\n",
       "      <td>177</td>\n",
       "      <td>1175</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>20</td>\n",
       "      <td>12968</td>\n",
       "      <td>5</td>\n",
       "      <td>6</td>\n",
       "      <td>1962</td>\n",
       "      <td>1962</td>\n",
       "      <td>737</td>\n",
       "      <td>0</td>\n",
       "      <td>175</td>\n",
       "      <td>912</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>20</td>\n",
       "      <td>10652</td>\n",
       "      <td>7</td>\n",
       "      <td>5</td>\n",
       "      <td>2006</td>\n",
       "      <td>2007</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1494</td>\n",
       "      <td>1494</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>20</td>\n",
       "      <td>10920</td>\n",
       "      <td>6</td>\n",
       "      <td>5</td>\n",
       "      <td>1960</td>\n",
       "      <td>1960</td>\n",
       "      <td>733</td>\n",
       "      <td>0</td>\n",
       "      <td>520</td>\n",
       "      <td>1253</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>45</td>\n",
       "      <td>6120</td>\n",
       "      <td>7</td>\n",
       "      <td>8</td>\n",
       "      <td>1929</td>\n",
       "      <td>2001</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>832</td>\n",
       "      <td>832</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>20</td>\n",
       "      <td>11241</td>\n",
       "      <td>6</td>\n",
       "      <td>7</td>\n",
       "      <td>1970</td>\n",
       "      <td>1970</td>\n",
       "      <td>578</td>\n",
       "      <td>0</td>\n",
       "      <td>426</td>\n",
       "      <td>1004</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>90</td>\n",
       "      <td>10791</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>1967</td>\n",
       "      <td>1967</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>20</td>\n",
       "      <td>13695</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>2004</td>\n",
       "      <td>2004</td>\n",
       "      <td>646</td>\n",
       "      <td>0</td>\n",
       "      <td>468</td>\n",
       "      <td>1114</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>20</td>\n",
       "      <td>7560</td>\n",
       "      <td>5</td>\n",
       "      <td>6</td>\n",
       "      <td>1958</td>\n",
       "      <td>1965</td>\n",
       "      <td>504</td>\n",
       "      <td>0</td>\n",
       "      <td>525</td>\n",
       "      <td>1029</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>60</td>\n",
       "      <td>14215</td>\n",
       "      <td>8</td>\n",
       "      <td>5</td>\n",
       "      <td>2005</td>\n",
       "      <td>2006</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1158</td>\n",
       "      <td>1158</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>45</td>\n",
       "      <td>7449</td>\n",
       "      <td>7</td>\n",
       "      <td>7</td>\n",
       "      <td>1930</td>\n",
       "      <td>1950</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>637</td>\n",
       "      <td>637</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>20</td>\n",
       "      <td>9742</td>\n",
       "      <td>8</td>\n",
       "      <td>5</td>\n",
       "      <td>2002</td>\n",
       "      <td>2002</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1777</td>\n",
       "      <td>1777</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>120</td>\n",
       "      <td>4224</td>\n",
       "      <td>5</td>\n",
       "      <td>7</td>\n",
       "      <td>1976</td>\n",
       "      <td>1976</td>\n",
       "      <td>840</td>\n",
       "      <td>0</td>\n",
       "      <td>200</td>\n",
       "      <td>1040</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>20</td>\n",
       "      <td>8246</td>\n",
       "      <td>5</td>\n",
       "      <td>8</td>\n",
       "      <td>1968</td>\n",
       "      <td>2001</td>\n",
       "      <td>188</td>\n",
       "      <td>668</td>\n",
       "      <td>204</td>\n",
       "      <td>1060</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>20</td>\n",
       "      <td>14230</td>\n",
       "      <td>8</td>\n",
       "      <td>5</td>\n",
       "      <td>2007</td>\n",
       "      <td>2007</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1566</td>\n",
       "      <td>1566</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>20</td>\n",
       "      <td>7200</td>\n",
       "      <td>5</td>\n",
       "      <td>7</td>\n",
       "      <td>1951</td>\n",
       "      <td>2000</td>\n",
       "      <td>234</td>\n",
       "      <td>486</td>\n",
       "      <td>180</td>\n",
       "      <td>900</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>20</td>\n",
       "      <td>11478</td>\n",
       "      <td>8</td>\n",
       "      <td>5</td>\n",
       "      <td>2007</td>\n",
       "      <td>2008</td>\n",
       "      <td>1218</td>\n",
       "      <td>0</td>\n",
       "      <td>486</td>\n",
       "      <td>1704</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>20</td>\n",
       "      <td>16321</td>\n",
       "      <td>5</td>\n",
       "      <td>6</td>\n",
       "      <td>1957</td>\n",
       "      <td>1997</td>\n",
       "      <td>1277</td>\n",
       "      <td>0</td>\n",
       "      <td>207</td>\n",
       "      <td>1484</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>30</td>\n",
       "      <td>6324</td>\n",
       "      <td>4</td>\n",
       "      <td>6</td>\n",
       "      <td>1927</td>\n",
       "      <td>1950</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>520</td>\n",
       "      <td>520</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1430</th>\n",
       "      <td>60</td>\n",
       "      <td>21930</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>2005</td>\n",
       "      <td>2005</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>732</td>\n",
       "      <td>732</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1431</th>\n",
       "      <td>120</td>\n",
       "      <td>4928</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>1976</td>\n",
       "      <td>1976</td>\n",
       "      <td>958</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>958</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1432</th>\n",
       "      <td>30</td>\n",
       "      <td>10800</td>\n",
       "      <td>4</td>\n",
       "      <td>6</td>\n",
       "      <td>1927</td>\n",
       "      <td>2007</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>656</td>\n",
       "      <td>656</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1433</th>\n",
       "      <td>60</td>\n",
       "      <td>10261</td>\n",
       "      <td>6</td>\n",
       "      <td>5</td>\n",
       "      <td>2000</td>\n",
       "      <td>2000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>936</td>\n",
       "      <td>936</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1434</th>\n",
       "      <td>20</td>\n",
       "      <td>17400</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>1977</td>\n",
       "      <td>1977</td>\n",
       "      <td>936</td>\n",
       "      <td>0</td>\n",
       "      <td>190</td>\n",
       "      <td>1126</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1435</th>\n",
       "      <td>20</td>\n",
       "      <td>8400</td>\n",
       "      <td>6</td>\n",
       "      <td>9</td>\n",
       "      <td>1962</td>\n",
       "      <td>2005</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1319</td>\n",
       "      <td>1319</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1436</th>\n",
       "      <td>20</td>\n",
       "      <td>9000</td>\n",
       "      <td>4</td>\n",
       "      <td>6</td>\n",
       "      <td>1971</td>\n",
       "      <td>1971</td>\n",
       "      <td>616</td>\n",
       "      <td>0</td>\n",
       "      <td>248</td>\n",
       "      <td>864</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1437</th>\n",
       "      <td>20</td>\n",
       "      <td>12444</td>\n",
       "      <td>8</td>\n",
       "      <td>5</td>\n",
       "      <td>2008</td>\n",
       "      <td>2008</td>\n",
       "      <td>1336</td>\n",
       "      <td>0</td>\n",
       "      <td>596</td>\n",
       "      <td>1932</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1438</th>\n",
       "      <td>20</td>\n",
       "      <td>7407</td>\n",
       "      <td>6</td>\n",
       "      <td>7</td>\n",
       "      <td>1957</td>\n",
       "      <td>1996</td>\n",
       "      <td>600</td>\n",
       "      <td>0</td>\n",
       "      <td>312</td>\n",
       "      <td>912</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1439</th>\n",
       "      <td>60</td>\n",
       "      <td>11584</td>\n",
       "      <td>7</td>\n",
       "      <td>6</td>\n",
       "      <td>1979</td>\n",
       "      <td>1979</td>\n",
       "      <td>315</td>\n",
       "      <td>110</td>\n",
       "      <td>114</td>\n",
       "      <td>539</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1440</th>\n",
       "      <td>70</td>\n",
       "      <td>11526</td>\n",
       "      <td>6</td>\n",
       "      <td>7</td>\n",
       "      <td>1922</td>\n",
       "      <td>1994</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>588</td>\n",
       "      <td>588</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1441</th>\n",
       "      <td>120</td>\n",
       "      <td>4426</td>\n",
       "      <td>6</td>\n",
       "      <td>5</td>\n",
       "      <td>2004</td>\n",
       "      <td>2004</td>\n",
       "      <td>697</td>\n",
       "      <td>0</td>\n",
       "      <td>151</td>\n",
       "      <td>848</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1442</th>\n",
       "      <td>60</td>\n",
       "      <td>11003</td>\n",
       "      <td>10</td>\n",
       "      <td>5</td>\n",
       "      <td>2008</td>\n",
       "      <td>2008</td>\n",
       "      <td>765</td>\n",
       "      <td>0</td>\n",
       "      <td>252</td>\n",
       "      <td>1017</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1443</th>\n",
       "      <td>30</td>\n",
       "      <td>8854</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>1916</td>\n",
       "      <td>1950</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>952</td>\n",
       "      <td>952</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1444</th>\n",
       "      <td>20</td>\n",
       "      <td>8500</td>\n",
       "      <td>7</td>\n",
       "      <td>5</td>\n",
       "      <td>2004</td>\n",
       "      <td>2004</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1422</td>\n",
       "      <td>1422</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1445</th>\n",
       "      <td>85</td>\n",
       "      <td>8400</td>\n",
       "      <td>6</td>\n",
       "      <td>5</td>\n",
       "      <td>1966</td>\n",
       "      <td>1966</td>\n",
       "      <td>187</td>\n",
       "      <td>627</td>\n",
       "      <td>0</td>\n",
       "      <td>814</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1446</th>\n",
       "      <td>20</td>\n",
       "      <td>26142</td>\n",
       "      <td>5</td>\n",
       "      <td>7</td>\n",
       "      <td>1962</td>\n",
       "      <td>1962</td>\n",
       "      <td>593</td>\n",
       "      <td>0</td>\n",
       "      <td>595</td>\n",
       "      <td>1188</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1447</th>\n",
       "      <td>60</td>\n",
       "      <td>10000</td>\n",
       "      <td>8</td>\n",
       "      <td>5</td>\n",
       "      <td>1995</td>\n",
       "      <td>1996</td>\n",
       "      <td>1079</td>\n",
       "      <td>0</td>\n",
       "      <td>141</td>\n",
       "      <td>1220</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1448</th>\n",
       "      <td>50</td>\n",
       "      <td>11767</td>\n",
       "      <td>4</td>\n",
       "      <td>7</td>\n",
       "      <td>1910</td>\n",
       "      <td>2000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>560</td>\n",
       "      <td>560</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1449</th>\n",
       "      <td>180</td>\n",
       "      <td>1533</td>\n",
       "      <td>5</td>\n",
       "      <td>7</td>\n",
       "      <td>1970</td>\n",
       "      <td>1970</td>\n",
       "      <td>553</td>\n",
       "      <td>0</td>\n",
       "      <td>77</td>\n",
       "      <td>630</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1450</th>\n",
       "      <td>90</td>\n",
       "      <td>9000</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>1974</td>\n",
       "      <td>1974</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>896</td>\n",
       "      <td>896</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1451</th>\n",
       "      <td>20</td>\n",
       "      <td>9262</td>\n",
       "      <td>8</td>\n",
       "      <td>5</td>\n",
       "      <td>2008</td>\n",
       "      <td>2009</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1573</td>\n",
       "      <td>1573</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1452</th>\n",
       "      <td>180</td>\n",
       "      <td>3675</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>2005</td>\n",
       "      <td>2005</td>\n",
       "      <td>547</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>547</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1453</th>\n",
       "      <td>20</td>\n",
       "      <td>17217</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>2006</td>\n",
       "      <td>2006</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1140</td>\n",
       "      <td>1140</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1454</th>\n",
       "      <td>20</td>\n",
       "      <td>7500</td>\n",
       "      <td>7</td>\n",
       "      <td>5</td>\n",
       "      <td>2004</td>\n",
       "      <td>2005</td>\n",
       "      <td>410</td>\n",
       "      <td>0</td>\n",
       "      <td>811</td>\n",
       "      <td>1221</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1455</th>\n",
       "      <td>60</td>\n",
       "      <td>7917</td>\n",
       "      <td>6</td>\n",
       "      <td>5</td>\n",
       "      <td>1999</td>\n",
       "      <td>2000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>953</td>\n",
       "      <td>953</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1456</th>\n",
       "      <td>20</td>\n",
       "      <td>13175</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>1978</td>\n",
       "      <td>1988</td>\n",
       "      <td>790</td>\n",
       "      <td>163</td>\n",
       "      <td>589</td>\n",
       "      <td>1542</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1457</th>\n",
       "      <td>70</td>\n",
       "      <td>9042</td>\n",
       "      <td>7</td>\n",
       "      <td>9</td>\n",
       "      <td>1941</td>\n",
       "      <td>2006</td>\n",
       "      <td>275</td>\n",
       "      <td>0</td>\n",
       "      <td>877</td>\n",
       "      <td>1152</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1458</th>\n",
       "      <td>20</td>\n",
       "      <td>9717</td>\n",
       "      <td>5</td>\n",
       "      <td>6</td>\n",
       "      <td>1950</td>\n",
       "      <td>1996</td>\n",
       "      <td>49</td>\n",
       "      <td>1029</td>\n",
       "      <td>0</td>\n",
       "      <td>1078</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1459</th>\n",
       "      <td>20</td>\n",
       "      <td>9937</td>\n",
       "      <td>5</td>\n",
       "      <td>6</td>\n",
       "      <td>1965</td>\n",
       "      <td>1965</td>\n",
       "      <td>830</td>\n",
       "      <td>290</td>\n",
       "      <td>136</td>\n",
       "      <td>1256</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1460 rows Ã— 159 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      MSSubClass  LotArea  OverallQual  OverallCond  YearBuilt  YearRemodAdd  \\\n",
       "0             60     8450            7            5       2003          2003   \n",
       "1             20     9600            6            8       1976          1976   \n",
       "2             60    11250            7            5       2001          2002   \n",
       "3             70     9550            7            5       1915          1970   \n",
       "4             60    14260            8            5       2000          2000   \n",
       "5             50    14115            5            5       1993          1995   \n",
       "6             20    10084            8            5       2004          2005   \n",
       "7             60    10382            7            6       1973          1973   \n",
       "8             50     6120            7            5       1931          1950   \n",
       "9            190     7420            5            6       1939          1950   \n",
       "10            20    11200            5            5       1965          1965   \n",
       "11            60    11924            9            5       2005          2006   \n",
       "12            20    12968            5            6       1962          1962   \n",
       "13            20    10652            7            5       2006          2007   \n",
       "14            20    10920            6            5       1960          1960   \n",
       "15            45     6120            7            8       1929          2001   \n",
       "16            20    11241            6            7       1970          1970   \n",
       "17            90    10791            4            5       1967          1967   \n",
       "18            20    13695            5            5       2004          2004   \n",
       "19            20     7560            5            6       1958          1965   \n",
       "20            60    14215            8            5       2005          2006   \n",
       "21            45     7449            7            7       1930          1950   \n",
       "22            20     9742            8            5       2002          2002   \n",
       "23           120     4224            5            7       1976          1976   \n",
       "24            20     8246            5            8       1968          2001   \n",
       "25            20    14230            8            5       2007          2007   \n",
       "26            20     7200            5            7       1951          2000   \n",
       "27            20    11478            8            5       2007          2008   \n",
       "28            20    16321            5            6       1957          1997   \n",
       "29            30     6324            4            6       1927          1950   \n",
       "...          ...      ...          ...          ...        ...           ...   \n",
       "1430          60    21930            5            5       2005          2005   \n",
       "1431         120     4928            6            6       1976          1976   \n",
       "1432          30    10800            4            6       1927          2007   \n",
       "1433          60    10261            6            5       2000          2000   \n",
       "1434          20    17400            5            5       1977          1977   \n",
       "1435          20     8400            6            9       1962          2005   \n",
       "1436          20     9000            4            6       1971          1971   \n",
       "1437          20    12444            8            5       2008          2008   \n",
       "1438          20     7407            6            7       1957          1996   \n",
       "1439          60    11584            7            6       1979          1979   \n",
       "1440          70    11526            6            7       1922          1994   \n",
       "1441         120     4426            6            5       2004          2004   \n",
       "1442          60    11003           10            5       2008          2008   \n",
       "1443          30     8854            6            6       1916          1950   \n",
       "1444          20     8500            7            5       2004          2004   \n",
       "1445          85     8400            6            5       1966          1966   \n",
       "1446          20    26142            5            7       1962          1962   \n",
       "1447          60    10000            8            5       1995          1996   \n",
       "1448          50    11767            4            7       1910          2000   \n",
       "1449         180     1533            5            7       1970          1970   \n",
       "1450          90     9000            5            5       1974          1974   \n",
       "1451          20     9262            8            5       2008          2009   \n",
       "1452         180     3675            5            5       2005          2005   \n",
       "1453          20    17217            5            5       2006          2006   \n",
       "1454          20     7500            7            5       2004          2005   \n",
       "1455          60     7917            6            5       1999          2000   \n",
       "1456          20    13175            6            6       1978          1988   \n",
       "1457          70     9042            7            9       1941          2006   \n",
       "1458          20     9717            5            6       1950          1996   \n",
       "1459          20     9937            5            6       1965          1965   \n",
       "\n",
       "      BsmtFinSF1  BsmtFinSF2  BsmtUnfSF  TotalBsmtSF          ...            \\\n",
       "0            706           0        150          856          ...             \n",
       "1            978           0        284         1262          ...             \n",
       "2            486           0        434          920          ...             \n",
       "3            216           0        540          756          ...             \n",
       "4            655           0        490         1145          ...             \n",
       "5            732           0         64          796          ...             \n",
       "6           1369           0        317         1686          ...             \n",
       "7            859          32        216         1107          ...             \n",
       "8              0           0        952          952          ...             \n",
       "9            851           0        140          991          ...             \n",
       "10           906           0        134         1040          ...             \n",
       "11           998           0        177         1175          ...             \n",
       "12           737           0        175          912          ...             \n",
       "13             0           0       1494         1494          ...             \n",
       "14           733           0        520         1253          ...             \n",
       "15             0           0        832          832          ...             \n",
       "16           578           0        426         1004          ...             \n",
       "17             0           0          0            0          ...             \n",
       "18           646           0        468         1114          ...             \n",
       "19           504           0        525         1029          ...             \n",
       "20             0           0       1158         1158          ...             \n",
       "21             0           0        637          637          ...             \n",
       "22             0           0       1777         1777          ...             \n",
       "23           840           0        200         1040          ...             \n",
       "24           188         668        204         1060          ...             \n",
       "25             0           0       1566         1566          ...             \n",
       "26           234         486        180          900          ...             \n",
       "27          1218           0        486         1704          ...             \n",
       "28          1277           0        207         1484          ...             \n",
       "29             0           0        520          520          ...             \n",
       "...          ...         ...        ...          ...          ...             \n",
       "1430           0           0        732          732          ...             \n",
       "1431         958           0          0          958          ...             \n",
       "1432           0           0        656          656          ...             \n",
       "1433           0           0        936          936          ...             \n",
       "1434         936           0        190         1126          ...             \n",
       "1435           0           0       1319         1319          ...             \n",
       "1436         616           0        248          864          ...             \n",
       "1437        1336           0        596         1932          ...             \n",
       "1438         600           0        312          912          ...             \n",
       "1439         315         110        114          539          ...             \n",
       "1440           0           0        588          588          ...             \n",
       "1441         697           0        151          848          ...             \n",
       "1442         765           0        252         1017          ...             \n",
       "1443           0           0        952          952          ...             \n",
       "1444           0           0       1422         1422          ...             \n",
       "1445         187         627          0          814          ...             \n",
       "1446         593           0        595         1188          ...             \n",
       "1447        1079           0        141         1220          ...             \n",
       "1448           0           0        560          560          ...             \n",
       "1449         553           0         77          630          ...             \n",
       "1450           0           0        896          896          ...             \n",
       "1451           0           0       1573         1573          ...             \n",
       "1452         547           0          0          547          ...             \n",
       "1453           0           0       1140         1140          ...             \n",
       "1454         410           0        811         1221          ...             \n",
       "1455           0           0        953          953          ...             \n",
       "1456         790         163        589         1542          ...             \n",
       "1457         275           0        877         1152          ...             \n",
       "1458          49        1029          0         1078          ...             \n",
       "1459         830         290        136         1256          ...             \n",
       "\n",
       "      SaleType_ConLw  SaleType_New  SaleType_Oth  SaleType_WD  \\\n",
       "0                  0             0             0            1   \n",
       "1                  0             0             0            1   \n",
       "2                  0             0             0            1   \n",
       "3                  0             0             0            1   \n",
       "4                  0             0             0            1   \n",
       "5                  0             0             0            1   \n",
       "6                  0             0             0            1   \n",
       "7                  0             0             0            1   \n",
       "8                  0             0             0            1   \n",
       "9                  0             0             0            1   \n",
       "10                 0             0             0            1   \n",
       "11                 0             1             0            0   \n",
       "12                 0             0             0            1   \n",
       "13                 0             1             0            0   \n",
       "14                 0             0             0            1   \n",
       "15                 0             0             0            1   \n",
       "16                 0             0             0            1   \n",
       "17                 0             0             0            1   \n",
       "18                 0             0             0            1   \n",
       "19                 0             0             0            0   \n",
       "20                 0             1             0            0   \n",
       "21                 0             0             0            1   \n",
       "22                 0             0             0            1   \n",
       "23                 0             0             0            1   \n",
       "24                 0             0             0            1   \n",
       "25                 0             0             0            1   \n",
       "26                 0             0             0            1   \n",
       "27                 0             0             0            1   \n",
       "28                 0             0             0            1   \n",
       "29                 0             0             0            1   \n",
       "...              ...           ...           ...          ...   \n",
       "1430               0             0             0            1   \n",
       "1431               0             0             0            1   \n",
       "1432               0             0             0            1   \n",
       "1433               0             0             0            1   \n",
       "1434               0             0             0            1   \n",
       "1435               0             0             0            0   \n",
       "1436               0             0             0            1   \n",
       "1437               0             1             0            0   \n",
       "1438               0             0             0            1   \n",
       "1439               0             0             0            1   \n",
       "1440               0             0             0            1   \n",
       "1441               0             0             0            1   \n",
       "1442               0             0             0            1   \n",
       "1443               0             0             0            1   \n",
       "1444               0             0             0            1   \n",
       "1445               0             0             0            1   \n",
       "1446               0             0             0            1   \n",
       "1447               0             0             0            1   \n",
       "1448               0             0             0            1   \n",
       "1449               0             0             0            1   \n",
       "1450               0             0             0            1   \n",
       "1451               0             1             0            0   \n",
       "1452               0             0             0            1   \n",
       "1453               0             0             0            1   \n",
       "1454               0             0             0            1   \n",
       "1455               0             0             0            1   \n",
       "1456               0             0             0            1   \n",
       "1457               0             0             0            1   \n",
       "1458               0             0             0            1   \n",
       "1459               0             0             0            1   \n",
       "\n",
       "      SaleCondition_Abnorml  SaleCondition_AdjLand  SaleCondition_Alloca  \\\n",
       "0                         0                      0                     0   \n",
       "1                         0                      0                     0   \n",
       "2                         0                      0                     0   \n",
       "3                         1                      0                     0   \n",
       "4                         0                      0                     0   \n",
       "5                         0                      0                     0   \n",
       "6                         0                      0                     0   \n",
       "7                         0                      0                     0   \n",
       "8                         1                      0                     0   \n",
       "9                         0                      0                     0   \n",
       "10                        0                      0                     0   \n",
       "11                        0                      0                     0   \n",
       "12                        0                      0                     0   \n",
       "13                        0                      0                     0   \n",
       "14                        0                      0                     0   \n",
       "15                        0                      0                     0   \n",
       "16                        0                      0                     0   \n",
       "17                        0                      0                     0   \n",
       "18                        0                      0                     0   \n",
       "19                        1                      0                     0   \n",
       "20                        0                      0                     0   \n",
       "21                        0                      0                     0   \n",
       "22                        0                      0                     0   \n",
       "23                        0                      0                     0   \n",
       "24                        0                      0                     0   \n",
       "25                        0                      0                     0   \n",
       "26                        0                      0                     0   \n",
       "27                        0                      0                     0   \n",
       "28                        0                      0                     0   \n",
       "29                        0                      0                     0   \n",
       "...                     ...                    ...                   ...   \n",
       "1430                      0                      0                     0   \n",
       "1431                      0                      0                     0   \n",
       "1432                      0                      0                     0   \n",
       "1433                      0                      0                     0   \n",
       "1434                      0                      0                     0   \n",
       "1435                      1                      0                     0   \n",
       "1436                      0                      0                     0   \n",
       "1437                      0                      0                     0   \n",
       "1438                      0                      0                     0   \n",
       "1439                      0                      0                     0   \n",
       "1440                      0                      0                     0   \n",
       "1441                      0                      0                     0   \n",
       "1442                      0                      0                     0   \n",
       "1443                      0                      0                     0   \n",
       "1444                      0                      0                     0   \n",
       "1445                      0                      0                     0   \n",
       "1446                      0                      0                     0   \n",
       "1447                      0                      0                     0   \n",
       "1448                      0                      0                     0   \n",
       "1449                      1                      0                     0   \n",
       "1450                      0                      0                     0   \n",
       "1451                      0                      0                     0   \n",
       "1452                      0                      0                     0   \n",
       "1453                      1                      0                     0   \n",
       "1454                      0                      0                     0   \n",
       "1455                      0                      0                     0   \n",
       "1456                      0                      0                     0   \n",
       "1457                      0                      0                     0   \n",
       "1458                      0                      0                     0   \n",
       "1459                      0                      0                     0   \n",
       "\n",
       "      SaleCondition_Family  SaleCondition_Normal  SaleCondition_Partial  \n",
       "0                        0                     1                      0  \n",
       "1                        0                     1                      0  \n",
       "2                        0                     1                      0  \n",
       "3                        0                     0                      0  \n",
       "4                        0                     1                      0  \n",
       "5                        0                     1                      0  \n",
       "6                        0                     1                      0  \n",
       "7                        0                     1                      0  \n",
       "8                        0                     0                      0  \n",
       "9                        0                     1                      0  \n",
       "10                       0                     1                      0  \n",
       "11                       0                     0                      1  \n",
       "12                       0                     1                      0  \n",
       "13                       0                     0                      1  \n",
       "14                       0                     1                      0  \n",
       "15                       0                     1                      0  \n",
       "16                       0                     1                      0  \n",
       "17                       0                     1                      0  \n",
       "18                       0                     1                      0  \n",
       "19                       0                     0                      0  \n",
       "20                       0                     0                      1  \n",
       "21                       0                     1                      0  \n",
       "22                       0                     1                      0  \n",
       "23                       0                     1                      0  \n",
       "24                       0                     1                      0  \n",
       "25                       0                     1                      0  \n",
       "26                       0                     1                      0  \n",
       "27                       0                     1                      0  \n",
       "28                       0                     1                      0  \n",
       "29                       0                     1                      0  \n",
       "...                    ...                   ...                    ...  \n",
       "1430                     0                     1                      0  \n",
       "1431                     0                     1                      0  \n",
       "1432                     0                     1                      0  \n",
       "1433                     0                     1                      0  \n",
       "1434                     0                     1                      0  \n",
       "1435                     0                     0                      0  \n",
       "1436                     0                     1                      0  \n",
       "1437                     0                     0                      1  \n",
       "1438                     0                     1                      0  \n",
       "1439                     0                     1                      0  \n",
       "1440                     0                     1                      0  \n",
       "1441                     0                     1                      0  \n",
       "1442                     0                     1                      0  \n",
       "1443                     0                     1                      0  \n",
       "1444                     0                     1                      0  \n",
       "1445                     0                     1                      0  \n",
       "1446                     0                     1                      0  \n",
       "1447                     0                     1                      0  \n",
       "1448                     0                     1                      0  \n",
       "1449                     0                     0                      0  \n",
       "1450                     0                     1                      0  \n",
       "1451                     0                     0                      1  \n",
       "1452                     0                     1                      0  \n",
       "1453                     0                     0                      0  \n",
       "1454                     0                     1                      0  \n",
       "1455                     0                     1                      0  \n",
       "1456                     0                     1                      0  \n",
       "1457                     0                     1                      0  \n",
       "1458                     0                     1                      0  \n",
       "1459                     0                     1                      0  \n",
       "\n",
       "[1460 rows x 159 columns]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "one_hot_encoded_training_predictors = pd.get_dummies(train_predictors)\n",
    "one_hot_encoded_training_predictors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Comparing one-hot encoded model vs non-categorical mode1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false,
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Absolute Error when Dropping Categoricals: 18424\n",
      "Mean Abslute Error with One-Hot Encoding: 18210\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "def get_mae(X, y):\n",
    "  # multiple by -1 to make positive MAE score instead of neg value returned as sklearn convention\n",
    "  return -1 * cross_val_score(RandomForestRegressor(50),\n",
    "                             X, y, scoring='neg_mean_absolute_error').mean()\n",
    "\n",
    "predictors_without_categoricals = train_predictors.select_dtypes(exclude=['object'])\n",
    "\n",
    "mae_without_categoricals = get_mae(predictors_without_categoricals, target)\n",
    "\n",
    "mae_one_hot_encoded = get_mae(one_hot_encoded_training_predictors, target)\n",
    "\n",
    "print('Mean Absolute Error when Dropping Categoricals: ' + str(int(mae_without_categoricals)))\n",
    "print('Mean Abslute Error with One-Hot Encoding: ' + str(int(mae_one_hot_encoded)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### Applying to Multiple Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "one_hot_encoded_training_predictors = pd.get_dummies(train_predictors)\n",
    "one_hot_encoded_test_predictors = pd.get_dummies(test_predictors)\n",
    "\n",
    "# This was cool, I was not aware of this\n",
    "final_train, final_test = one_hot_encoded_training_predictors.align(\n",
    "  one_hot_encoded_test_predictors, join='left', axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "* The align command makes sure the columns show up in the same order in both datasets (it uses column names to identify which columns line up in each dataset.) The argument join='left' specifies that we will do the equivalent of SQL's left join. That means, if there are ever columns that show up in one dataset and not the other, we will keep exactly the columns from our training data. The argument join='inner' would do what SQL databases call an inner join, keeping only the columns showing up in both datasets. That's also a sensible choice.\n",
    "\n",
    "### One-Hot Encoding Conclusion\n",
    "\n",
    "* Pipelines: Deploying models into production ready systems is a topic unto itself. While one-hot encoding is still a great approach, your code will need to built in an especially robust way. Scikit-learn pipelines are a great tool for this. Scikit-learn offers a class for one-hot encoding and this can be added to a Pipeline. Unfortunately, it doesn't handle text or object values, which is a common use case.\n",
    "\n",
    "\n",
    "* Applications To Text for Deep Learning: Keras and TensorFlow have fuctionality for one-hot encoding, which is useful for working with text.\n",
    "\n",
    "\n",
    "* Categoricals with Many Values: Scikit-learn's FeatureHasher uses the hashing trick to store high-dimensional data. This will add some complexity to your modeling code."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# <center> Learning to Use XGBoost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "\n",
    "Source: https://www.kaggle.com/dansbecker/learning-to-use-xgboost\n",
    "\n",
    "* XGBoost is the leading model for working with standard tabular data (the type of data you store in Pandas DataFrames, as opposed to more exotic types of data like images and videos). XGBoost models dominate many Kaggle competitions.\n",
    "\n",
    "\n",
    "* To reach peak accuracy, XGBoost models require more knowledge and model tuning than techniques like Random Forest. \n",
    "\n",
    "* XGBoost is an implementation of the Gradient Boosted Decision Trees algorithm (scikit-learn has another version of this algorithm, but XGBoost has some technical advantages.) What is Gradient Boosted Decision Trees? We'll walk through a diagram.\n",
    "\n",
    "### [Image Here]\n",
    "\n",
    "* We go through cycles that repeatedly builds new models and combines them into an ensemble model. We start the cycle by calculating the errors for each observation in the dataset. We then build a new model to predict those. We add predictions from this error-predicting model to the \"ensemble of models.\"\n",
    "\n",
    "* To make a prediction, we add the predictions from all previous models. We can use these predictions to calculate new errors, build the next model, and add it to the ensemble.\n",
    "\n",
    "* There's one piece outside that cycle. We need some base prediction to start the cycle. In practice, the initial predictions can be pretty naive. Even if it's predictions are wildly inaccurate, subsequent additions to the ensemble will address those errors.\n",
    "\n",
    "* This process may sound complicated, but the code to use it is straightforward. We'll fill in some additional explanatory details in the model tuning section below.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### Example - XG Boost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Example Setup\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import Imputer\n",
    "\n",
    "data = pd.read_csv('Data/train.csv')\n",
    "data.dropna(axis=0, subset=['SalePrice'], inplace=True)\n",
    "y = data.SalePrice\n",
    "X = data.drop(['SalePrice'], axis=1).select_dtypes(exclude=['object'])\n",
    "train_X, test_X, train_y, test_y = train_test_split(X.as_matrix(), y.as_matrix(), test_size=0.25)\n",
    "\n",
    "my_imputer = Imputer()\n",
    "train_X = my_imputer.fit_transform(train_X)\n",
    "test_X = my_imputer.transform(test_X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Question: What dose the verbose attribute do in the fit function?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false,
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
       "       colsample_bytree=1, gamma=0, learning_rate=0.1, max_delta_step=0,\n",
       "       max_depth=3, min_child_weight=1, missing=None, n_estimators=100,\n",
       "       n_jobs=1, nthread=None, objective='reg:linear', random_state=0,\n",
       "       reg_alpha=0, reg_lambda=1, scale_pos_weight=1, seed=None,\n",
       "       silent=True, subsample=1)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from xgboost import XGBRegressor\n",
    "\n",
    "my_model = XGBRegressor()\n",
    "# Add silent=True to avoid printing out updates with each cycle\n",
    "my_model.fit(train_X, train_y, verbose=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false,
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Absolute Error:16623.5142337\n"
     ]
    }
   ],
   "source": [
    "# make predictions \n",
    "predictions = my_model.predict(test_X)\n",
    "\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "print('Mean Absolute Error:' + str(mean_absolute_error(predictions, test_y)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "##### Model Tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "hidden": true
   },
   "source": [
    "* XGBoost has a few parameters that can dramatically affect your model's accuracy and training speed. The first parameters you should understand are: n_estimators and early_stopping_rounds\n",
    "\n",
    "* n_estimators specifies how many times to go through the modeling cycle described above.\n",
    "\n",
    "\n",
    "[Image]\n",
    "\n",
    "* In the underfitting vs overfitting graph, n_estimators moves you further to the right. Too low a value causes underfitting, which is inaccurate predictions on both training data and new data. Too large a value causes overfitting, which is accurate predictions on training data, but inaccurate predictions on new data (which is what we care about). You can experiment with your dataset to find the ideal. Typical values range from 100-1000, though this depends a lot on the learning rate discussed below\n",
    "\n",
    "\n",
    "* The argument early_stopping_rounds offers a way to automatically find the ideal value. Early stopping causes the model to stop iterating when the validation score stops improving, even if we aren't at the hard stop for n_estimators. It's smart to set a high value for n_estimators and then use early_stopping_rounds to find the optimal time to stop iterating.\n",
    "\n",
    "\n",
    "* Since random chance sometimes causes a single round where validation scores don't improve, you need to specify a number for how many rounds of straight deterioration to allow before stopping. early_stopping_rounds = 5 is a reasonable value. Thus we stop after 5 straight rounds of deteriorating validation scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false,
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
       "       colsample_bytree=1, gamma=0, learning_rate=0.1, max_delta_step=0,\n",
       "       max_depth=3, min_child_weight=1, missing=None, n_emtimators=1000,\n",
       "       n_estimators=100, n_jobs=1, nthread=None, objective='reg:linear',\n",
       "       random_state=0, reg_alpha=0, reg_lambda=1, scale_pos_weight=1,\n",
       "       seed=None, silent=True, subsample=1)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_model = XGBRegressor(n_emtimators=1000)\n",
    "my_model.fit(train_X, train_y, early_stopping_rounds=5,\n",
    "            eval_set=[(test_X, test_y)], verbose=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "* When using early_stopping_rounds, you need to set aside some of your data for checking the number of rounds to use. If you later want to fit a model with all of your data, set n_estimators to whatever value you found to be optimal when run with early stopping.\n",
    "\n",
    "* Learning Rate: Instead of getting predictions by simply adding up the predictions from each component model, we will multiply the predictions from each model by a small number before adding them in. This means each tree we add to the ensemble helps us less. In practice, this reduces the model's propensity to overfit.\n",
    "\n",
    "* So, you can use a higher value of n_estimators without overfitting. If you use early stopping, the appropriate number of trees will be set automatically.\n",
    "\n",
    "* In general, a small learning rate (and large number of estimators) will yield more accurate XGBoost models, though it will also take the model longer to train since it does more iterations through the cycle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false,
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
       "       colsample_bytree=1, gamma=0, learning_rate=0.05, max_delta_step=0,\n",
       "       max_depth=3, min_child_weight=1, missing=None, n_estimators=1000,\n",
       "       n_jobs=1, nthread=None, objective='reg:linear', random_state=0,\n",
       "       reg_alpha=0, reg_lambda=1, scale_pos_weight=1, seed=None,\n",
       "       silent=True, subsample=1)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_model = XGBRegressor(n_estimators=1000, learning_rate=0.05)\n",
    "my_model.fit(train_X, train_y, early_stopping_rounds=5,\n",
    "            eval_set=[(test_X, test_y)], verbose=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "* n_jobs: On larger datasets where runtime is a consideration, you can use parallelism to build your models faster. It's common to set the parameter n_jobs equal to the number of cores on your machine. On smaller datasets, this won't help.\n",
    "\n",
    "\n",
    "* The resulting model won't be any better, so micro-optimizing for fitting time is typically nothing but a distraction. But, it's useful in large datasets where you would otherwise spend a long time waiting during the fit command."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# <center> Partial Dependence Plots"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "\n",
    "Source: Partial dependence plots show how each variable or predictor affects the model's predictions\n",
    "\n",
    "* Partial dependence plots show how each variable or predictor affects the model's predictions\n",
    "\n",
    "* The partial dependence plot is calculated only after the model has been fit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<matplotlib.figure.Figure at 0x113aaa400>,\n",
       " [<matplotlib.axes._subplots.AxesSubplot at 0x113ac70b8>,\n",
       "  <matplotlib.axes._subplots.AxesSubplot at 0x114b86d30>])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAcQAAADPCAYAAABr76FoAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nrkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x113aaa400>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn.ensemble import GradientBoostingRegressor, GradientBoostingClassifier\n",
    "from sklearn.ensemble.partial_dependence import partial_dependence, plot_partial_dependence\n",
    "from sklearn.preprocessing import Imputer\n",
    "\n",
    "cols_to_use = ['Distance', 'Landsize', 'BuildingArea']\n",
    "\n",
    "def get_some_data():\n",
    "    data = pd.read_csv('Data/melb_data.csv')\n",
    "    y = data.Price\n",
    "    X = data[cols_to_use]\n",
    "    my_imputer = Imputer()\n",
    "    imputed_X = my_imputer.fit_transform(X)\n",
    "    return imputed_X, y\n",
    "    \n",
    "X, y = get_some_data()\n",
    "my_model = GradientBoostingRegressor()\n",
    "my_model.fit(X, y)\n",
    "\n",
    "my_plts = plot_partial_dependence(my_model, features=[0,2],\n",
    "                                 X=X, feature_names=cols_to_use, \n",
    "                                 grid_resolution=10)\n",
    "\n",
    "my_plts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "hidden": true
   },
   "source": [
    "Some tips related to plot_partial_dependence:\n",
    "\n",
    "* The features are the column numbers from the X array or dataframe that you wish to have plotted. This starts to look bad beyond 2 or 3 variables. You could make repeated calls to plot 2 or 3 at a time.\n",
    "\n",
    "\n",
    "* There are options to establish what points on the horizontal axis are plotted. The simplest is grid_resolution which we use to determine how many different points are plotted. These plots tend to look jagged as that value increases, because you will pick up lots of randomness or noise in your model. It's best not to take the small or jagged fluctuations too literally. Smaller values of grid_resolution smooth this out. It's also much less of an issue for datasets with many rows.\n",
    "\n",
    "\n",
    "\n",
    "* There is a function called partial_dependence to get the raw data making up this plot, rather than making the visual plot itself. This is useful if you want to control how it is visualized using a plotting package like Seaborn. With moderate effort, you could make much nicer looking plots."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### Another Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAa4AAADPCAYAAACtKjXRAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1136e64a8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "titanic_data = pd.read_csv('Data/titanic.csv')\n",
    "titanic_y = titanic_data.Survived\n",
    "clf = GradientBoostingClassifier()\n",
    "titanic_X_colns = ['PassengerId','Age','Fare',]\n",
    "titanic_X = titanic_data[titanic_X_coln]\n",
    "my_imputer = Imputer()\n",
    "imputed_titanic_X = my_imputer.fit_transform(titanic_X)\n",
    "\n",
    "clf.fit(imputed_titanic_X, titanic_y)\n",
    "titanic_plots = plot_partial_dependence(clf, features=[1,2], X=imputed_titanic_X,\n",
    "                                       feature_names=titanic_X_colns, grid_resolution=8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "These might seem surprising at first glance. But they show some interesting insights:\n",
    "\n",
    "* Being young increased your odds of survival. This is consistent with historical recountings that they got women and children off the Titanic first.\n",
    "\n",
    "\n",
    "* People who paid more had better odds of survival. It turns out that higher fares got you a cabin that was closer to the top of the boat, and may have given you better odds of getting a life-boat."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# <center> Pipelines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Source: https://www.kaggle.com/dansbecker/pipelines\n",
    "\n",
    "Pipelines are a simple way to keep your data processing and modeling code organized. Specifically, a pipeline bundles preprocessing and modeling steps so you can use the whole bundle as if it were a single step.\n",
    "\n",
    "\n",
    "Many data scientists hack together models without pipelines, but Pipelines have some important benefits. Those include:\n",
    "\n",
    "* Cleaner Code: You won't need to keep track of your training (and validation) data at each step of processing. Accounting for data at each step of processing can get messy. With a pipeline, you don't need to manually keep track of each step.\n",
    "\n",
    "\n",
    "* Fewer Bugs: There are fewer opportunities to mis-apply a step or forget a pre-processing step.\n",
    "\n",
    "\n",
    "* Easier to Productionize: It can be surprisingly hard to transition a model from a prototype to something deployable at scale. We won't go into the many related concerns here, but pipelines can help.\n",
    "\n",
    "\n",
    "* More Options For Model Testing: You will see an example in the next tutorial, which covers cross-validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Read Data\n",
    "data = pd.read_csv('Data/melb_data.csv')\n",
    "cols_to_use = ['Rooms', 'Distance', 'Landsize', 'BuildingArea', 'YearBuilt']\n",
    "X = data[cols_to_use]\n",
    "y = data.Price\n",
    "train_X, test_X, train_y, test_y = train_test_split(X,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import Imputer\n",
    "\n",
    "my_pipeline = make_pipeline(Imputer(), RandomForestRegressor())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "my_pipeline.fit(train_X, train_y)\n",
    "predictions = my_pipeline.predict(test_X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# <center> Cross Validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "\n",
    "Source: https://www.kaggle.com/dansbecker/cross-validation\n",
    "### Trade-offs\n",
    "\n",
    "* Cross-validation gives a more accurate measure of model quality, which is especially important if you are making a lot of modeling decisions. However, it can take more time to run, because it estimates models once for each fold. So it is doing more total work.\n",
    "\n",
    "\n",
    "* For the same reasons, a simple train-test split is sufficient for larger datasets. It will run faster, and you may have enough data that there's little need to re-use some of it for holdout.\n",
    "\n",
    "\n",
    "* There's no simple threshold for what constitutes a large vs small dataset. If your model takes a couple minute or less to run, it's probably worth switching to cross-validation. If your model takes much longer to run, cross-validation may slow down your workflow more than it's worth."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "data = pd.read_csv('Data/melb_data.csv')\n",
    "cols_to_use = ['Rooms', 'Distance', 'Landsize', 'BuildingArea', 'YearBuilt']\n",
    "X = data[cols_to_use]\n",
    "y = data.Price"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import Imputer\n",
    "my_pipeline = make_pipeline(Imputer(), RandomForestRegressor())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false,
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-321975.14698542 -303865.03126415 -283660.67469611]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "scores = cross_val_score(my_pipeline, X, y, scoring='neg_mean_absolute_error')\n",
    "print(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false,
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Absolute Error 303166.950982\n"
     ]
    }
   ],
   "source": [
    "print('Mean Absolute Error %2f' %(-1 * scores.mean()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-14T01:41:11.974624Z",
     "start_time": "2018-02-14T01:41:11.972368Z"
    },
    "heading_collapsed": true
   },
   "source": [
    "# <center> Data Leakage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "hidden": true
   },
   "source": [
    "Source: https://www.kaggle.com/dansbecker/data-leakage\n",
    "### What is Data Leakage\n",
    "\n",
    "* Leakage: specifically, leakage causes a model to look accurate until you start making decisions with the model, and then the model becomes very inaccurate\n",
    "\n",
    "### Leaky Predictors\n",
    "\n",
    "* This occurs when your predictors include data that will not be available at the time you make predictions.\n",
    "\n",
    "\n",
    "* People take antibiotic medicines after getting pneumonia in order to recover. So the raw data shows a strong relationship between those columns. But took_antibiotic_medicine is frequently changed after the value for got_pneumonia is determined. This is target leakage. The model would see that anyone who has a value of False for took_antibiotic_medicine didn't have pneumonia. Validation data comes from the same source, so the pattern will repeat itself in validation, and the model will have great validation (or cross-validation) scores. But the model will be very inaccurate when subsequently deployed in the real world. To prevent this type of data leakage, any variable updated (or created) after the target value is realized should be excluded. Because when we use this model to make new predictions, that data won't be available to the model.\n",
    "\n",
    "### Leaky Validation Strategy\n",
    "\n",
    "*  much different type of leak occurs when you aren't careful distinguishing training data from validation data. For example, this happens if you run preprocessing (like fitting the Imputer for missing values) before calling train_test_split. Validation is meant to be a measure of how the model does on data it hasn't considered before. You can corrupt this process in subtle ways if the validation data affects the preprocessing behavoir.. The end result? Your model will get very good validation scores, giving you great confidence in it, but perform poorly when you deploy it to make decisions.\n",
    "\n",
    "### Preventing Leaky Predictors\n",
    "* However, leaky predictors frequently have high statistical correlations to the target. So two tactics to keep in mind:\n",
    "\n",
    "    - To screen for possible leaky predictors, look for columns that are statistically correlated to your target.\n",
    "    - If you build a model and find it extremely accurate, you likely have a leakage problem.\n",
    "    \n",
    "### Preventing Leaky Validation Strategies\n",
    "* If your validation is based on a simple train-test split, exclude the validation data from any type of fitting, including the fitting of preprocessing steps. This is easier if you use scikit-learn Pipelines. When using cross-validation, it's even more critical that you use pipelines and do your preprocessing inside the pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-14T01:35:34.581522Z",
     "start_time": "2018-02-14T01:35:34.568191Z"
    },
    "collapsed": false,
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   card  reports       age  income     share  expenditure  owner  selfemp  \\\n",
      "0  True        0  37.66667  4.5200  0.033270   124.983300   True    False   \n",
      "1  True        0  33.25000  2.4200  0.005217     9.854167  False    False   \n",
      "2  True        0  33.66667  4.5000  0.004156    15.000000   True    False   \n",
      "3  True        0  30.50000  2.5400  0.065214   137.869200  False    False   \n",
      "4  True        0  32.16667  9.7867  0.067051   546.503300   True    False   \n",
      "\n",
      "   dependents  months  majorcards  active  \n",
      "0           3      54           1      12  \n",
      "1           3      34           1      13  \n",
      "2           4      58           1       5  \n",
      "3           0      25           1       7  \n",
      "4           2      64           1       5  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "data = pd.read_csv('Data/AER_credit_card_data.csv', \n",
    "                   true_values = ['yes'],\n",
    "                   false_values = ['no'])\n",
    "print(data.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-14T01:36:01.094496Z",
     "start_time": "2018-02-14T01:36:01.085546Z"
    },
    "collapsed": false,
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1319, 12)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-14T01:36:02.349853Z",
     "start_time": "2018-02-14T01:36:01.784637Z"
    },
    "collapsed": false,
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross-val accuracy: 0.981043\n"
     ]
    }
   ],
   "source": [
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "y = data.card\n",
    "X = data.drop(['card'], axis=1)\n",
    "\n",
    "# Since there was no preprocessing, we didn't need a pipeline here. Used anyway as best practice\n",
    "modeling_pipeline = make_pipeline(RandomForestClassifier())\n",
    "cv_scores = cross_val_score(modeling_pipeline, X, y, scoring='accuracy')\n",
    "print(\"Cross-val accuracy: %f\" %cv_scores.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-14T01:36:03.349664Z",
     "start_time": "2018-02-14T01:36:03.341284Z"
    },
    "collapsed": false,
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fraction of those who received a card with no expenditures: 0.02\n",
      "Fraction of those who received a card with no expenditures: 1.00\n"
     ]
    }
   ],
   "source": [
    "expenditures_cardholders = data.expenditure[data.card]\n",
    "expenditures_noncardholders = data.expenditure[~data.card]\n",
    "\n",
    "print('Fraction of those who received a card with no expenditures: %.2f' \\\n",
    "      %(( expenditures_cardholders == 0).mean()))\n",
    "print('Fraction of those who received a card with no expenditures: %.2f' \\\n",
    "      %((expenditures_noncardholders == 0).mean()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-14T01:38:34.784209Z",
     "start_time": "2018-02-14T01:38:34.714796Z"
    },
    "collapsed": false,
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross-val accuracy: 0.808192\n"
     ]
    }
   ],
   "source": [
    "potential_leaks = ['expenditure', 'share', 'active', 'majorcards']\n",
    "X2 = X.drop(potential_leaks, axis=1)\n",
    "cv_scores = cross_val_score(modeling_pipeline, X2, y, scoring='accuracy')\n",
    "print(\"Cross-val accuracy: %f\" %cv_scores.mean())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
