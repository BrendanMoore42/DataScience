{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Kaggle Learn\n",
    "#### Machine Learning Level 2\n",
    "\n",
    "<br>Source: https://www.kaggle.com/learn/machine-learning\n",
    "<br>Slide Command: jupyter nbconvert Presentation.ipynb --to slides --post serve\n",
    "<br><br>\n",
    "\n",
    "Presentor: Kavi Sekhon\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### L1: Missing Value Introduction\n",
    "Source: https://www.kaggle.com/dansbecker/handling-missing-values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-13T09:36:53.919394Z",
     "start_time": "2018-03-13T09:36:53.896116Z"
    },
    "collapsed": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# Import pandas and our training data\n",
    "import pandas as pd\n",
    "main_file_path = 'Data/train.csv'\n",
    "data = pd.read_csv(main_file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-13T09:36:53.932543Z",
     "start_time": "2018-03-13T09:36:53.920806Z"
    },
    "collapsed": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Id                  0\n",
       "MSSubClass          0\n",
       "MSZoning            0\n",
       "LotFrontage       259\n",
       "LotArea             0\n",
       "Street              0\n",
       "Alley            1369\n",
       "LotShape            0\n",
       "LandContour         0\n",
       "Utilities           0\n",
       "LotConfig           0\n",
       "LandSlope           0\n",
       "Neighborhood        0\n",
       "Condition1          0\n",
       "Condition2          0\n",
       "BldgType            0\n",
       "HouseStyle          0\n",
       "OverallQual         0\n",
       "OverallCond         0\n",
       "YearBuilt           0\n",
       "YearRemodAdd        0\n",
       "RoofStyle           0\n",
       "RoofMatl            0\n",
       "Exterior1st         0\n",
       "Exterior2nd         0\n",
       "MasVnrType          8\n",
       "MasVnrArea          8\n",
       "ExterQual           0\n",
       "ExterCond           0\n",
       "Foundation          0\n",
       "                 ... \n",
       "BedroomAbvGr        0\n",
       "KitchenAbvGr        0\n",
       "KitchenQual         0\n",
       "TotRmsAbvGrd        0\n",
       "Functional          0\n",
       "Fireplaces          0\n",
       "FireplaceQu       690\n",
       "GarageType         81\n",
       "GarageYrBlt        81\n",
       "GarageFinish       81\n",
       "GarageCars          0\n",
       "GarageArea          0\n",
       "GarageQual         81\n",
       "GarageCond         81\n",
       "PavedDrive          0\n",
       "WoodDeckSF          0\n",
       "OpenPorchSF         0\n",
       "EnclosedPorch       0\n",
       "3SsnPorch           0\n",
       "ScreenPorch         0\n",
       "PoolArea            0\n",
       "PoolQC           1453\n",
       "Fence            1179\n",
       "MiscFeature      1406\n",
       "MiscVal             0\n",
       "MoSold              0\n",
       "YrSold              0\n",
       "SaleType            0\n",
       "SaleCondition       0\n",
       "SalePrice           0\n",
       "Length: 81, dtype: int64"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# View the total sum of null values\n",
    "data.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-13T03:50:32.188729Z",
     "start_time": "2018-03-13T03:50:32.186541Z"
    },
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Null Value Solutions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "\n",
    "##### 1) A Simple Option: Drop Columns with Missing Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-13T09:36:53.936712Z",
     "start_time": "2018-03-13T09:36:53.934226Z"
    },
    "collapsed": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# Copying the data into a pandas dataframe\n",
    "original_data = pd.DataFrame(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-13T09:36:53.966300Z",
     "start_time": "2018-03-13T09:36:53.938404Z"
    },
    "collapsed": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>MSSubClass</th>\n",
       "      <th>MSZoning</th>\n",
       "      <th>LotArea</th>\n",
       "      <th>Street</th>\n",
       "      <th>LotShape</th>\n",
       "      <th>LandContour</th>\n",
       "      <th>Utilities</th>\n",
       "      <th>LotConfig</th>\n",
       "      <th>LandSlope</th>\n",
       "      <th>...</th>\n",
       "      <th>EnclosedPorch</th>\n",
       "      <th>3SsnPorch</th>\n",
       "      <th>ScreenPorch</th>\n",
       "      <th>PoolArea</th>\n",
       "      <th>MiscVal</th>\n",
       "      <th>MoSold</th>\n",
       "      <th>YrSold</th>\n",
       "      <th>SaleType</th>\n",
       "      <th>SaleCondition</th>\n",
       "      <th>SalePrice</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>60</td>\n",
       "      <td>RL</td>\n",
       "      <td>8450</td>\n",
       "      <td>Pave</td>\n",
       "      <td>Reg</td>\n",
       "      <td>Lvl</td>\n",
       "      <td>AllPub</td>\n",
       "      <td>Inside</td>\n",
       "      <td>Gtl</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>2008</td>\n",
       "      <td>WD</td>\n",
       "      <td>Normal</td>\n",
       "      <td>208500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>20</td>\n",
       "      <td>RL</td>\n",
       "      <td>9600</td>\n",
       "      <td>Pave</td>\n",
       "      <td>Reg</td>\n",
       "      <td>Lvl</td>\n",
       "      <td>AllPub</td>\n",
       "      <td>FR2</td>\n",
       "      <td>Gtl</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>2007</td>\n",
       "      <td>WD</td>\n",
       "      <td>Normal</td>\n",
       "      <td>181500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>60</td>\n",
       "      <td>RL</td>\n",
       "      <td>11250</td>\n",
       "      <td>Pave</td>\n",
       "      <td>IR1</td>\n",
       "      <td>Lvl</td>\n",
       "      <td>AllPub</td>\n",
       "      <td>Inside</td>\n",
       "      <td>Gtl</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>9</td>\n",
       "      <td>2008</td>\n",
       "      <td>WD</td>\n",
       "      <td>Normal</td>\n",
       "      <td>223500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>70</td>\n",
       "      <td>RL</td>\n",
       "      <td>9550</td>\n",
       "      <td>Pave</td>\n",
       "      <td>IR1</td>\n",
       "      <td>Lvl</td>\n",
       "      <td>AllPub</td>\n",
       "      <td>Corner</td>\n",
       "      <td>Gtl</td>\n",
       "      <td>...</td>\n",
       "      <td>272</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>2006</td>\n",
       "      <td>WD</td>\n",
       "      <td>Abnorml</td>\n",
       "      <td>140000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>60</td>\n",
       "      <td>RL</td>\n",
       "      <td>14260</td>\n",
       "      <td>Pave</td>\n",
       "      <td>IR1</td>\n",
       "      <td>Lvl</td>\n",
       "      <td>AllPub</td>\n",
       "      <td>FR2</td>\n",
       "      <td>Gtl</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>12</td>\n",
       "      <td>2008</td>\n",
       "      <td>WD</td>\n",
       "      <td>Normal</td>\n",
       "      <td>250000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 62 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Id  MSSubClass MSZoning  LotArea Street LotShape LandContour Utilities  \\\n",
       "0   1          60       RL     8450   Pave      Reg         Lvl    AllPub   \n",
       "1   2          20       RL     9600   Pave      Reg         Lvl    AllPub   \n",
       "2   3          60       RL    11250   Pave      IR1         Lvl    AllPub   \n",
       "3   4          70       RL     9550   Pave      IR1         Lvl    AllPub   \n",
       "4   5          60       RL    14260   Pave      IR1         Lvl    AllPub   \n",
       "\n",
       "  LotConfig LandSlope    ...    EnclosedPorch 3SsnPorch ScreenPorch PoolArea  \\\n",
       "0    Inside       Gtl    ...                0         0           0        0   \n",
       "1       FR2       Gtl    ...                0         0           0        0   \n",
       "2    Inside       Gtl    ...                0         0           0        0   \n",
       "3    Corner       Gtl    ...              272         0           0        0   \n",
       "4       FR2       Gtl    ...                0         0           0        0   \n",
       "\n",
       "  MiscVal  MoSold  YrSold  SaleType  SaleCondition SalePrice  \n",
       "0       0       2    2008        WD         Normal    208500  \n",
       "1       0       5    2007        WD         Normal    181500  \n",
       "2       0       9    2008        WD         Normal    223500  \n",
       "3       0       2    2006        WD        Abnorml    140000  \n",
       "4       0      12    2008        WD         Normal    250000  \n",
       "\n",
       "[5 rows x 62 columns]"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Dropping any row with a missing value\n",
    "data_without_missing_values = original_data.dropna(axis=1)\n",
    "data_without_missing_values.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "##### 2) A Better Option: Imputation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-13T03:37:10.762520Z",
     "start_time": "2018-03-13T03:37:10.757526Z"
    },
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "* Imputation fills in the missing value with some number. The imputed value won't be exactly right in most cases, but it usually gives more accurate models than dropping the column entirely.\n",
    "* The default behavior fills in the mean value for imputation. Statisticians have researched more complex strategies, but those complex strategies typically give no benefit once you plug the results into sophisticated machine learning models.\n",
    "* One (of many) nice things about Imputation is that it can be included in a scikit-learn Pipeline. Pipelines simplify model building, model validation and model deployment.\n",
    "\n",
    "* Warning: Sklearn imputation only work on numeric values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-13T09:36:53.979053Z",
     "start_time": "2018-03-13T09:36:53.967846Z"
    },
    "collapsed": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  1.00000000e+00,   6.00000000e+01,   6.50000000e+01, ...,\n",
       "          2.00000000e+00,   2.00800000e+03,   2.08500000e+05],\n",
       "       [  2.00000000e+00,   2.00000000e+01,   8.00000000e+01, ...,\n",
       "          5.00000000e+00,   2.00700000e+03,   1.81500000e+05],\n",
       "       [  3.00000000e+00,   6.00000000e+01,   6.80000000e+01, ...,\n",
       "          9.00000000e+00,   2.00800000e+03,   2.23500000e+05],\n",
       "       ..., \n",
       "       [  1.45800000e+03,   7.00000000e+01,   6.60000000e+01, ...,\n",
       "          5.00000000e+00,   2.01000000e+03,   2.66500000e+05],\n",
       "       [  1.45900000e+03,   2.00000000e+01,   6.80000000e+01, ...,\n",
       "          4.00000000e+00,   2.01000000e+03,   1.42125000e+05],\n",
       "       [  1.46000000e+03,   2.00000000e+01,   7.50000000e+01, ...,\n",
       "          6.00000000e+00,   2.00800000e+03,   1.47500000e+05]])"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Creating a list of numerical columns\n",
    "numeric_columns  = list(original_data.select_dtypes(include=[int, float]).columns)\n",
    "\n",
    "# Import the imputer class from sklearn\n",
    "from sklearn.preprocessing import Imputer\n",
    "\n",
    "# Assign the Imputer to a variable\n",
    "my_imputer =  Imputer()\n",
    "\n",
    "# Fit and transform numerical columns with imputed values\n",
    "data_with_imputed_values = my_imputer.fit_transform(original_data[numeric_columns])\n",
    "\n",
    "# View the imputated values\n",
    "data_with_imputed_values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "##### 3) An Extention to Imputation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-13T09:36:53.983896Z",
     "start_time": "2018-03-13T09:36:53.980352Z"
    },
    "collapsed": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# make a copy to avoid changing original data (when Imputing)\n",
    "new_data = original_data[numeric_columns].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-13T09:36:54.003426Z",
     "start_time": "2018-03-13T09:36:53.985234Z"
    },
    "collapsed": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# Finding features with missing null values\n",
    "cols_with_missing = (col for col in new_data.columns\n",
    "                    if new_data[col].isnull().any())\n",
    "\n",
    "# creating pandas dataframe with only missing null values\n",
    "for col in cols_with_missing:\n",
    "  new_data[col + '_was missing'] = new_data[col].isnull()\n",
    "  \n",
    "# Imputating featurse with missing data\n",
    "my_imputer = Imputer()\n",
    "new_data = my_imputer.fit_transform(new_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Comparing All Null Value Solution\n",
    "\n",
    "##### Basic Problem Set-up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-13T09:36:54.079144Z",
     "start_time": "2018-03-13T09:36:54.004764Z"
    },
    "collapsed": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# Import pandas and sklean modules\n",
    "import pandas as pd\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Import data\n",
    "melb_data = pd.read_csv('Data/melb_data.csv')\n",
    "\n",
    "# Set target, and predictors\n",
    "melb_target = melb_data.Price\n",
    "melb_predictors = melb_data.drop(['Price'], axis=1)\n",
    "\n",
    "# For the sake of keeping the example simple, we'll use only numeric predictors.\n",
    "melb_numeric_predictors = melb_predictors.select_dtypes(exclude=['object'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "##### Creating Function to Measure Quality of An Approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-13T09:36:54.090819Z",
     "start_time": "2018-03-13T09:36:54.080489Z"
    },
    "collapsed": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# Import RandomForestModel, Mean Absolute Error, and Train-Test Split\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Split the data by train-test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(melb_numeric_predictors, \n",
    "                                                    melb_target,\n",
    "                                                    train_size=0.7, \n",
    "                                                    test_size=0.3, \n",
    "                                                    random_state=0)\n",
    "\n",
    "# Creating a function to score our model that includeds imputated data\n",
    "def score_dataset(X_train, X_test, y_train, y_test):\n",
    "  model = RandomForestRegressor()\n",
    "  model.fit(X_train, y_train)\n",
    "  preds = model.predict(X_test)\n",
    "  return (mean_absolute_error(y_test, preds))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "##### 1) Get Model Score from Dropping Columns with Missing Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-13T09:36:54.301566Z",
     "start_time": "2018-03-13T09:36:54.092105Z"
    },
    "collapsed": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Absolute Error form dropping columsn with Missing Values:\n",
      "352643.478529\n"
     ]
    }
   ],
   "source": [
    "# Generating a list of column for columns with missing data\n",
    "cols_with_missing = [col for col in X_train.columns \n",
    "                     if X_train[col].isnull().any()]\n",
    "\n",
    "# Ceating predictor dataframe without columns that contain missing values\n",
    "reduced_X_train = X_train.drop(cols_with_missing, axis=1)\n",
    "reduced_X_test = X_test.drop(cols_with_missing, axis=1)\n",
    "\n",
    "# Using our scoring function to print out our accuracy measure\n",
    "print('Mean Absolute Error form dropping columsn with Missing Values:')\n",
    "print(score_dataset(reduced_X_train, reduced_X_test, y_train, y_test))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "\n",
    "##### 2) Get Model Score from imputation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-13T09:36:55.004757Z",
     "start_time": "2018-03-13T09:36:54.303490Z"
    },
    "collapsed": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Absolute Error from Impuation:\n",
      "202816.389092\n"
     ]
    }
   ],
   "source": [
    "# Creating testing and training data using Imputation\n",
    "from sklearn.preprocessing import Imputer\n",
    " \n",
    "my_impture = Imputer()\n",
    "imputed_X_train = my_imputer.fit_transform(X_train)\n",
    "imputed_X_test = my_imputer.transform(X_test)\n",
    "\n",
    "\n",
    "# Using our scoring function to print out our accuracy measure for Imputation\n",
    "print('Mean Absolute Error from Impuation:')\n",
    "print(score_dataset(imputed_X_train, imputed_X_test, y_train, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "##### 3) Get Score from Imputation with Extra Columns Showing What Was Imputed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-13T09:36:55.856728Z",
     "start_time": "2018-03-13T09:36:55.006348Z"
    },
    "collapsed": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Absolute Error from Imputation while Track What Was Imputed:\n",
      "205427.982678\n"
     ]
    }
   ],
   "source": [
    "# Copy our imputed training and test data from the last example\n",
    "imputed_X_train_plus = X_train.copy()\n",
    "imputed_X_test_plus = X_test.copy()\n",
    "\n",
    "# Generating a list of columns that containe missing values\n",
    "cols_with_missing =  (col for col in X_train.columns\n",
    "                     if X_train[col].isnull().any())\n",
    "\n",
    "# creating a seperate list of columns of null values in the train and test data\n",
    "for col in cols_with_missing:\n",
    "  imputed_X_train_plus[col + '_was_missing'] = imputed_X_train_plus[col].isnull()\n",
    "  imputed_X_test_plus[col + '_was_missing'] = imputed_X_test_plus[col].isnull()\n",
    "  \n",
    "  \n",
    "# Impuation fitting and transforming data for both train and test set\n",
    "my_imputer = Imputer()\n",
    "imputed_X_train_plus = my_imputer.fit_transform(imputed_X_train_plus)\n",
    "imputed_X_test_plus = my_imputer.fit_transform(imputed_X_test_plus)\n",
    "\n",
    "print(\"Mean Absolute Error from Imputation while Track What Was Imputed:\")\n",
    "print(score_dataset(imputed_X_train_plus, imputed_X_test_plus, y_train, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "* In this case, the extension didn't make a big difference. As mentioned before, this can vary widely from one dataset to the next (largely determined by whether rows with missing values are intrinsically like or unlike those without missing values)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### L2: Using Categorical Data with One Hot Encoding\n",
    "Source: https://www.kaggle.com/dansbecker/using-categorical-data-with-one-hot-encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-13T09:36:55.919706Z",
     "start_time": "2018-03-13T09:36:55.858476Z"
    },
    "collapsed": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# Read the data\n",
    "import pandas as pd\n",
    "train_data = pd.read_csv('Data/train.csv')\n",
    "test_data = pd.read_csv('Data/test.csv')\n",
    "\n",
    "# Drop the houses where the target is missing\n",
    "train_data.dropna(axis=0, subset=['SalePrice'], inplace=True)\n",
    "\n",
    "# Creating the target series\n",
    "target = train_data.SalePrice\n",
    "\n",
    "\n",
    "# Generating a list of columns that containe missing values\n",
    "cols_with_missing = [col for col in train_data.columns\n",
    "                    if train_data[col].isnull().any()]\n",
    "\n",
    "# Creatinp the training and testing dataframe\n",
    "candidate_train_predictors = train_data.drop(['Id', 'SalePrice'] + cols_with_missing, axis=1)\n",
    "candidate_test_predictors = test_data.drop(['Id'] + cols_with_missing, axis=1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-13T09:36:55.927379Z",
     "start_time": "2018-03-13T09:36:55.921127Z"
    },
    "collapsed": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GrLivArea        int64\n",
       "Fireplaces       int64\n",
       "Foundation      object\n",
       "BldgType        object\n",
       "BsmtFullBath     int64\n",
       "Functional      object\n",
       "KitchenQual     object\n",
       "BsmtFinSF1       int64\n",
       "LotArea          int64\n",
       "BsmtHalfBath     int64\n",
       "dtype: object"
      ]
     },
     "execution_count": 142,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Tangent: Dataframe Dtypes\n",
    "\n",
    "train_predictors.dtypes.sample(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-13T09:36:55.948600Z",
     "start_time": "2018-03-13T09:36:55.928910Z"
    },
    "collapsed": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# creating a list of colums that had dtype equal to object\n",
    "# and number of unique values in series is less than 10\n",
    "low_cardinality_cols = [cname for cname in candidate_train_predictors.columns if \n",
    "                                candidate_train_predictors[cname].nunique() < 10 and\n",
    "                                candidate_train_predictors[cname].dtype == \"object\"]\n",
    "\n",
    "# creating a list of colums that use either a int of float dtype\n",
    "numeric_cols = [cname for cname in candidate_train_predictors.columns if \n",
    "                                candidate_train_predictors[cname].dtype in ['int64', 'float64']]\n",
    "\n",
    "# creating a list columns including numrecial and low cardinality features\n",
    "my_cols = low_cardinality_cols + numeric_cols\n",
    "\n",
    "# creating the trainin data\n",
    "train_predictors = candidate_train_predictors[my_cols]\n",
    "\n",
    "# creating the testing data\n",
    "test_predictors = candidate_test_predictors[my_cols]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-13T09:36:55.969552Z",
     "start_time": "2018-03-13T09:36:55.949904Z"
    },
    "collapsed": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>MSZoning</th>\n",
       "      <th>Street</th>\n",
       "      <th>LotShape</th>\n",
       "      <th>LandContour</th>\n",
       "      <th>Utilities</th>\n",
       "      <th>LotConfig</th>\n",
       "      <th>LandSlope</th>\n",
       "      <th>Condition1</th>\n",
       "      <th>Condition2</th>\n",
       "      <th>BldgType</th>\n",
       "      <th>...</th>\n",
       "      <th>GarageArea</th>\n",
       "      <th>WoodDeckSF</th>\n",
       "      <th>OpenPorchSF</th>\n",
       "      <th>EnclosedPorch</th>\n",
       "      <th>3SsnPorch</th>\n",
       "      <th>ScreenPorch</th>\n",
       "      <th>PoolArea</th>\n",
       "      <th>MiscVal</th>\n",
       "      <th>MoSold</th>\n",
       "      <th>YrSold</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>RL</td>\n",
       "      <td>Pave</td>\n",
       "      <td>Reg</td>\n",
       "      <td>Lvl</td>\n",
       "      <td>AllPub</td>\n",
       "      <td>Inside</td>\n",
       "      <td>Gtl</td>\n",
       "      <td>Norm</td>\n",
       "      <td>Norm</td>\n",
       "      <td>1Fam</td>\n",
       "      <td>...</td>\n",
       "      <td>548</td>\n",
       "      <td>0</td>\n",
       "      <td>61</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>2008</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>RL</td>\n",
       "      <td>Pave</td>\n",
       "      <td>Reg</td>\n",
       "      <td>Lvl</td>\n",
       "      <td>AllPub</td>\n",
       "      <td>FR2</td>\n",
       "      <td>Gtl</td>\n",
       "      <td>Feedr</td>\n",
       "      <td>Norm</td>\n",
       "      <td>1Fam</td>\n",
       "      <td>...</td>\n",
       "      <td>460</td>\n",
       "      <td>298</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>2007</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>RL</td>\n",
       "      <td>Pave</td>\n",
       "      <td>IR1</td>\n",
       "      <td>Lvl</td>\n",
       "      <td>AllPub</td>\n",
       "      <td>Inside</td>\n",
       "      <td>Gtl</td>\n",
       "      <td>Norm</td>\n",
       "      <td>Norm</td>\n",
       "      <td>1Fam</td>\n",
       "      <td>...</td>\n",
       "      <td>608</td>\n",
       "      <td>0</td>\n",
       "      <td>42</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>9</td>\n",
       "      <td>2008</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>RL</td>\n",
       "      <td>Pave</td>\n",
       "      <td>IR1</td>\n",
       "      <td>Lvl</td>\n",
       "      <td>AllPub</td>\n",
       "      <td>Corner</td>\n",
       "      <td>Gtl</td>\n",
       "      <td>Norm</td>\n",
       "      <td>Norm</td>\n",
       "      <td>1Fam</td>\n",
       "      <td>...</td>\n",
       "      <td>642</td>\n",
       "      <td>0</td>\n",
       "      <td>35</td>\n",
       "      <td>272</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>2006</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>RL</td>\n",
       "      <td>Pave</td>\n",
       "      <td>IR1</td>\n",
       "      <td>Lvl</td>\n",
       "      <td>AllPub</td>\n",
       "      <td>FR2</td>\n",
       "      <td>Gtl</td>\n",
       "      <td>Norm</td>\n",
       "      <td>Norm</td>\n",
       "      <td>1Fam</td>\n",
       "      <td>...</td>\n",
       "      <td>836</td>\n",
       "      <td>192</td>\n",
       "      <td>84</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>12</td>\n",
       "      <td>2008</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 57 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "  MSZoning Street LotShape LandContour Utilities LotConfig LandSlope  \\\n",
       "0       RL   Pave      Reg         Lvl    AllPub    Inside       Gtl   \n",
       "1       RL   Pave      Reg         Lvl    AllPub       FR2       Gtl   \n",
       "2       RL   Pave      IR1         Lvl    AllPub    Inside       Gtl   \n",
       "3       RL   Pave      IR1         Lvl    AllPub    Corner       Gtl   \n",
       "4       RL   Pave      IR1         Lvl    AllPub       FR2       Gtl   \n",
       "\n",
       "  Condition1 Condition2 BldgType  ...   GarageArea WoodDeckSF OpenPorchSF  \\\n",
       "0       Norm       Norm     1Fam  ...          548          0          61   \n",
       "1      Feedr       Norm     1Fam  ...          460        298           0   \n",
       "2       Norm       Norm     1Fam  ...          608          0          42   \n",
       "3       Norm       Norm     1Fam  ...          642          0          35   \n",
       "4       Norm       Norm     1Fam  ...          836        192          84   \n",
       "\n",
       "  EnclosedPorch 3SsnPorch ScreenPorch PoolArea MiscVal MoSold YrSold  \n",
       "0             0         0           0        0       0      2   2008  \n",
       "1             0         0           0        0       0      5   2007  \n",
       "2             0         0           0        0       0      9   2008  \n",
       "3           272         0           0        0       0      2   2006  \n",
       "4             0         0           0        0       0     12   2008  \n",
       "\n",
       "[5 rows x 57 columns]"
      ]
     },
     "execution_count": 144,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### current dataframe\n",
    "train_predictors.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-13T09:36:55.978107Z",
     "start_time": "2018-03-13T09:36:55.972828Z"
    },
    "collapsed": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number for features: 57\n"
     ]
    }
   ],
   "source": [
    "### printing number of feature train_predictors\n",
    "print('Number for features:',train_predictors.shape[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Using One Hot Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-13T09:36:56.032044Z",
     "start_time": "2018-03-13T09:36:55.980037Z"
    },
    "collapsed": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>MSSubClass</th>\n",
       "      <th>LotArea</th>\n",
       "      <th>OverallQual</th>\n",
       "      <th>OverallCond</th>\n",
       "      <th>YearBuilt</th>\n",
       "      <th>YearRemodAdd</th>\n",
       "      <th>BsmtFinSF1</th>\n",
       "      <th>BsmtFinSF2</th>\n",
       "      <th>BsmtUnfSF</th>\n",
       "      <th>TotalBsmtSF</th>\n",
       "      <th>...</th>\n",
       "      <th>SaleType_ConLw</th>\n",
       "      <th>SaleType_New</th>\n",
       "      <th>SaleType_Oth</th>\n",
       "      <th>SaleType_WD</th>\n",
       "      <th>SaleCondition_Abnorml</th>\n",
       "      <th>SaleCondition_AdjLand</th>\n",
       "      <th>SaleCondition_Alloca</th>\n",
       "      <th>SaleCondition_Family</th>\n",
       "      <th>SaleCondition_Normal</th>\n",
       "      <th>SaleCondition_Partial</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>60</td>\n",
       "      <td>8450</td>\n",
       "      <td>7</td>\n",
       "      <td>5</td>\n",
       "      <td>2003</td>\n",
       "      <td>2003</td>\n",
       "      <td>706</td>\n",
       "      <td>0</td>\n",
       "      <td>150</td>\n",
       "      <td>856</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>20</td>\n",
       "      <td>9600</td>\n",
       "      <td>6</td>\n",
       "      <td>8</td>\n",
       "      <td>1976</td>\n",
       "      <td>1976</td>\n",
       "      <td>978</td>\n",
       "      <td>0</td>\n",
       "      <td>284</td>\n",
       "      <td>1262</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>60</td>\n",
       "      <td>11250</td>\n",
       "      <td>7</td>\n",
       "      <td>5</td>\n",
       "      <td>2001</td>\n",
       "      <td>2002</td>\n",
       "      <td>486</td>\n",
       "      <td>0</td>\n",
       "      <td>434</td>\n",
       "      <td>920</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>70</td>\n",
       "      <td>9550</td>\n",
       "      <td>7</td>\n",
       "      <td>5</td>\n",
       "      <td>1915</td>\n",
       "      <td>1970</td>\n",
       "      <td>216</td>\n",
       "      <td>0</td>\n",
       "      <td>540</td>\n",
       "      <td>756</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>60</td>\n",
       "      <td>14260</td>\n",
       "      <td>8</td>\n",
       "      <td>5</td>\n",
       "      <td>2000</td>\n",
       "      <td>2000</td>\n",
       "      <td>655</td>\n",
       "      <td>0</td>\n",
       "      <td>490</td>\n",
       "      <td>1145</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 159 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   MSSubClass  LotArea  OverallQual  OverallCond  YearBuilt  YearRemodAdd  \\\n",
       "0          60     8450            7            5       2003          2003   \n",
       "1          20     9600            6            8       1976          1976   \n",
       "2          60    11250            7            5       2001          2002   \n",
       "3          70     9550            7            5       1915          1970   \n",
       "4          60    14260            8            5       2000          2000   \n",
       "\n",
       "   BsmtFinSF1  BsmtFinSF2  BsmtUnfSF  TotalBsmtSF          ...            \\\n",
       "0         706           0        150          856          ...             \n",
       "1         978           0        284         1262          ...             \n",
       "2         486           0        434          920          ...             \n",
       "3         216           0        540          756          ...             \n",
       "4         655           0        490         1145          ...             \n",
       "\n",
       "   SaleType_ConLw  SaleType_New  SaleType_Oth  SaleType_WD  \\\n",
       "0               0             0             0            1   \n",
       "1               0             0             0            1   \n",
       "2               0             0             0            1   \n",
       "3               0             0             0            1   \n",
       "4               0             0             0            1   \n",
       "\n",
       "   SaleCondition_Abnorml  SaleCondition_AdjLand  SaleCondition_Alloca  \\\n",
       "0                      0                      0                     0   \n",
       "1                      0                      0                     0   \n",
       "2                      0                      0                     0   \n",
       "3                      1                      0                     0   \n",
       "4                      0                      0                     0   \n",
       "\n",
       "   SaleCondition_Family  SaleCondition_Normal  SaleCondition_Partial  \n",
       "0                     0                     1                      0  \n",
       "1                     0                     1                      0  \n",
       "2                     0                     1                      0  \n",
       "3                     0                     0                      0  \n",
       "4                     0                     1                      0  \n",
       "\n",
       "[5 rows x 159 columns]"
      ]
     },
     "execution_count": 146,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "one_hot_encoded_training_predictors = pd.get_dummies(train_predictors)\n",
    "one_hot_encoded_training_predictors.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Comparing one-hot encoded model vs non-categorical mode1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-13T09:36:58.998975Z",
     "start_time": "2018-03-13T09:36:56.033811Z"
    },
    "collapsed": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Absolute Error when Dropping Categoricals: 18490\n",
      "Mean Abslute Error with One-Hot Encoding: 18238\n"
     ]
    }
   ],
   "source": [
    "# Importing RandomForestRegressor, and cross val score\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "# A quick function that return the our accuracy metrics for given training values \n",
    "def get_mae(X, y):\n",
    "  return -1 * cross_val_score(RandomForestRegressor(50),\n",
    "                             X, y, scoring='neg_mean_absolute_error').mean()\n",
    "\n",
    "# creating a dataframe with numeical values\n",
    "predictors_without_categoricals = train_predictors.select_dtypes(exclude=['object'])\n",
    "\n",
    "# Viewing mae with and without one-host encoding using a RandomForestRegressor\n",
    "mae_without_categoricals = get_mae(predictors_without_categoricals, target)\n",
    "mae_one_hot_encoded = get_mae(one_hot_encoded_training_predictors, target)\n",
    "\n",
    "print('Mean Absolute Error when Dropping Categoricals: ' + str(int(mae_without_categoricals)))\n",
    "print('Mean Abslute Error with One-Hot Encoding: ' + str(int(mae_one_hot_encoded)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Applying to Multiple Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-13T09:36:59.042839Z",
     "start_time": "2018-03-13T09:36:59.000426Z"
    },
    "collapsed": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# Encoding training and testing data seperately\n",
    "one_hot_encoded_training_predictors = pd.get_dummies(train_predictors)\n",
    "one_hot_encoded_test_predictors = pd.get_dummies(test_predictors)\n",
    "\n",
    "# This was cool, I was not aware of this\n",
    "final_train, final_test = one_hot_encoded_training_predictors.align(\n",
    "  one_hot_encoded_test_predictors, join='left', axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "* The align command makes sure the columns show up in the same order in both datasets (it uses column names to identify which columns line up in each dataset.) The argument join='left' specifies that we will do the equivalent of SQL's left join. That means, if there are ever columns that show up in one dataset and not the other, we will keep exactly the columns from our training data. The argument join='inner' would do what SQL databases call an inner join, keeping only the columns showing up in both datasets. That's also a sensible choice.\n",
    "\n",
    "### One-Hot Encoding Conclusion\n",
    "\n",
    "* Pipelines: Deploying models into production ready systems is a topic unto itself. While one-hot encoding is still a great approach, your code will need to built in an especially robust way. Scikit-learn pipelines are a great tool for this. Scikit-learn offers a class for one-hot encoding and this can be added to a Pipeline. Unfortunately, it doesn't handle text or object values, which is a common use case.\n",
    "\n",
    "\n",
    "* Applications To Text for Deep Learning: Keras and TensorFlow have fuctionality for one-hot encoding, which is useful for working with text.\n",
    "\n",
    "\n",
    "* Categoricals with Many Values: Scikit-learn's FeatureHasher uses the hashing trick to store high-dimensional data. This will add some complexity to your modeling code."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# <center> L3: Learning to Use XGBoost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "\n",
    "Source: https://www.kaggle.com/dansbecker/learning-to-use-xgboost\n",
    "\n",
    "* XGBoost is the leading model for working with standard tabular data (the type of data you store in Pandas DataFrames, as opposed to more exotic types of data like images and videos). XGBoost models dominate many Kaggle competitions.\n",
    "\n",
    "\n",
    "* To reach peak accuracy, XGBoost models require more knowledge and model tuning than techniques like Random Forest. \n",
    "\n",
    "* XGBoost is an implementation of the Gradient Boosted Decision Trees algorithm (scikit-learn has another version of this algorithm, but XGBoost has some technical advantages.) What is Gradient Boosted Decision Trees? We'll walk through a diagram.\n",
    "\n",
    "### [Image Here]\n",
    "\n",
    "* We go through cycles that repeatedly builds new models and combines them into an ensemble model. We start the cycle by calculating the errors for each observation in the dataset. We then build a new model to predict those. We add predictions from this error-predicting model to the \"ensemble of models.\"\n",
    "\n",
    "* To make a prediction, we add the predictions from all previous models. We can use these predictions to calculate new errors, build the next model, and add it to the ensemble.\n",
    "\n",
    "* There's one piece outside that cycle. We need some base prediction to start the cycle. In practice, the initial predictions can be pretty naive. Even if it's predictions are wildly inaccurate, subsequent additions to the ensemble will address those errors.\n",
    "\n",
    "* This process may sound complicated, but the code to use it is straightforward. We'll fill in some additional explanatory details in the model tuning section below.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Example - XG Boost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-13T09:36:59.074933Z",
     "start_time": "2018-03-13T09:36:59.044408Z"
    },
    "collapsed": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# Model Setup\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import Imputer\n",
    "\n",
    "# Import Data, set Target & predictators\n",
    "data = pd.read_csv('Data/train.csv')\n",
    "data.dropna(axis=0, subset=['SalePrice'], inplace=True)\n",
    "y = data.SalePrice\n",
    "X = data.drop(['SalePrice'], axis=1).select_dtypes(exclude=['object'])\n",
    "train_X, test_X, train_y, test_y = train_test_split(X.as_matrix(), y.as_matrix(), test_size=0.25)\n",
    "\n",
    "# Imputate training and testing data\n",
    "my_imputer = Imputer()\n",
    "train_X = my_imputer.fit_transform(train_X)\n",
    "test_X = my_imputer.transform(test_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-13T09:36:59.252366Z",
     "start_time": "2018-03-13T09:36:59.076388Z"
    },
    "collapsed": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
       "       colsample_bytree=1, gamma=0, learning_rate=0.1, max_delta_step=0,\n",
       "       max_depth=3, min_child_weight=1, missing=None, n_estimators=100,\n",
       "       n_jobs=1, nthread=None, objective='reg:linear', random_state=0,\n",
       "       reg_alpha=0, reg_lambda=1, scale_pos_weight=1, seed=None,\n",
       "       silent=True, subsample=1)"
      ]
     },
     "execution_count": 150,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Importing the XGB Regressor from XGBoost\n",
    "from xgboost import XGBRegressor\n",
    "\n",
    "# Assiinging my model\n",
    "my_model = XGBRegressor()\n",
    "\n",
    "# I set the verbose to True, because logs are cool, initially\n",
    "my_model.fit(train_X, train_y, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-13T09:36:59.258993Z",
     "start_time": "2018-03-13T09:36:59.253777Z"
    },
    "collapsed": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Absolute Error:16682.41814\n"
     ]
    }
   ],
   "source": [
    "# make predictions on training dataset\n",
    "predictions = my_model.predict(test_X)\n",
    "\n",
    "# print mae for training set \n",
    "from sklearn.metrics import mean_absolute_error\n",
    "print('Mean Absolute Error:' + str(mean_absolute_error(predictions, test_y)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "##### Model Tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "* XGBoost has a few parameters that can dramatically affect your model's accuracy and training speed. The first parameters you should understand are: n_estimators and early_stopping_rounds\n",
    "\n",
    "* n_estimators specifies how many times to go through the modeling cycle described above.\n",
    "\n",
    "\n",
    "[Image]\n",
    "\n",
    "* In the underfitting vs overfitting graph, n_estimators moves you further to the right. Too low a value causes underfitting, which is inaccurate predictions on both training data and new data. Too large a value causes overfitting, which is accurate predictions on training data, but inaccurate predictions on new data (which is what we care about). You can experiment with your dataset to find the ideal. Typical values range from 100-1000, though this depends a lot on the learning rate discussed below\n",
    "\n",
    "\n",
    "* The argument early_stopping_rounds offers a way to automatically find the ideal value. Early stopping causes the model to stop iterating when the validation score stops improving, even if we aren't at the hard stop for n_estimators. It's smart to set a high value for n_estimators and then use early_stopping_rounds to find the optimal time to stop iterating.\n",
    "\n",
    "\n",
    "* Since random chance sometimes causes a single round where validation scores don't improve, you need to specify a number for how many rounds of straight deterioration to allow before stopping. early_stopping_rounds = 5 is a reasonable value. Thus we stop after 5 straight rounds of deteriorating validation scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-13T09:36:59.397396Z",
     "start_time": "2018-03-13T09:36:59.260588Z"
    },
    "collapsed": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
       "       colsample_bytree=1, gamma=0, learning_rate=0.1, max_delta_step=0,\n",
       "       max_depth=3, min_child_weight=1, missing=None, n_emtimators=1000,\n",
       "       n_estimators=100, n_jobs=1, nthread=None, objective='reg:linear',\n",
       "       random_state=0, reg_alpha=0, reg_lambda=1, scale_pos_weight=1,\n",
       "       seed=None, silent=True, subsample=1)"
      ]
     },
     "execution_count": 152,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Fitting model parameters n_esimators\n",
    "my_model = XGBRegressor(n_emtimators=1000)\n",
    "my_model.fit(train_X, train_y, early_stopping_rounds=5,\n",
    "            eval_set=[(test_X, test_y)], verbose=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "* When using early_stopping_rounds, you need to set aside some of your data for checking the number of rounds to use. If you later want to fit a model with all of your data, set n_estimators to whatever value you found to be optimal when run with early stopping.\n",
    "\n",
    "* Learning Rate: Instead of getting predictions by simply adding up the predictions from each component model, we will multiply the predictions from each model by a small number before adding them in. This means each tree we add to the ensemble helps us less. In practice, this reduces the model's propensity to overfit.\n",
    "\n",
    "* So, you can use a higher value of n_estimators without overfitting. If you use early stopping, the appropriate number of trees will be set automatically.\n",
    "\n",
    "* In general, a small learning rate (and large number of estimators) will yield more accurate XGBoost models, though it will also take the model longer to train since it does more iterations through the cycle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-13T09:36:59.558851Z",
     "start_time": "2018-03-13T09:36:59.398802Z"
    },
    "collapsed": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
       "       colsample_bytree=1, gamma=0, learning_rate=0.05, max_delta_step=0,\n",
       "       max_depth=3, min_child_weight=1, missing=None, n_estimators=1000,\n",
       "       n_jobs=1, nthread=None, objective='reg:linear', random_state=0,\n",
       "       reg_alpha=0, reg_lambda=1, scale_pos_weight=1, seed=None,\n",
       "       silent=True, subsample=1)"
      ]
     },
     "execution_count": 153,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Fitting model parameters with a learning rate\n",
    "my_model = XGBRegressor(n_estimators=1000, learning_rate=0.05)\n",
    "my_model.fit(train_X, train_y, early_stopping_rounds=5,\n",
    "            eval_set=[(test_X, test_y)], verbose=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "* n_jobs: On larger datasets where runtime is a consideration, you can use parallelism to build your models faster. It's common to set the parameter n_jobs equal to the number of cores on your machine. On smaller datasets, this won't help.\n",
    "\n",
    "\n",
    "* The resulting model won't be any better, so micro-optimizing for fitting time is typically nothing but a distraction. But, it's useful in large datasets where you would otherwise spend a long time waiting during the fit command."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# <center> L4: Partial Dependence Plots\n",
    "\n",
    "Source: https://www.kaggle.com/dansbecker/partial-dependence-plots\n",
    "\n",
    "* Partial dependence plots show how each variable or predictor affects the model's predictions\n",
    "\n",
    "* The partial dependence plot is calculated only after the model has been fit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-13T09:37:00.102160Z",
     "start_time": "2018-03-13T09:36:59.560228Z"
    },
    "collapsed": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<matplotlib.figure.Figure at 0x11b514828>,\n",
       " [<matplotlib.axes._subplots.AxesSubplot at 0x11b519630>,\n",
       "  <matplotlib.axes._subplots.AxesSubplot at 0x1142b3eb8>])"
      ]
     },
     "execution_count": 154,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAcQAAADPCAYAAABr76FoAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x11b514828>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Jupyter Command sto show visualization inline, after the cell\n",
    "%matplotlib inline\n",
    "\n",
    "# Import modules\n",
    "import pandas as pd\n",
    "from sklearn.ensemble import GradientBoostingRegressor, GradientBoostingClassifier\n",
    "from sklearn.ensemble.partial_dependence import partial_dependence, plot_partial_dependence\n",
    "from sklearn.preprocessing import Imputer\n",
    "\n",
    "# feature selcetion of our interested columns\n",
    "cols_to_use = ['Distance', 'Landsize', 'BuildingArea']\n",
    "\n",
    "# Creating a fucntion that call some data\n",
    "def get_some_data():\n",
    "    data = pd.read_csv('Data/melb_data.csv')\n",
    "    y = data.Price\n",
    "    X = data[cols_to_use]\n",
    "    my_imputer = Imputer()\n",
    "    imputed_X = my_imputer.fit_transform(X)\n",
    "    return imputed_X, y\n",
    "\n",
    "# Return training data from \"get_some_data\" function\n",
    "X, y = get_some_data()\n",
    "\n",
    "# Use the GradienBoostReqgressor\n",
    "my_model = GradientBoostingRegressor()\n",
    "\n",
    "# Fit the model\n",
    "my_model.fit(X, y)\n",
    "\n",
    "# Creating Partial Dependence plots with SKlearn\n",
    "my_plts = plot_partial_dependence(my_model, features=[0,2],\n",
    "                                 X=X, feature_names=cols_to_use, \n",
    "                                 grid_resolution=10)\n",
    "# View my plots\n",
    "my_plts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "Some tips related to plot_partial_dependence:\n",
    "\n",
    "* The features are the column numbers from the X array or dataframe that you wish to have plotted. This starts to look bad beyond 2 or 3 variables. You could make repeated calls to plot 2 or 3 at a time.\n",
    "\n",
    "\n",
    "* There are options to establish what points on the horizontal axis are plotted. The simplest is grid_resolution which we use to determine how many different points are plotted. These plots tend to look jagged as that value increases, because you will pick up lots of randomness or noise in your model. It's best not to take the small or jagged fluctuations too literally. Smaller values of grid_resolution smooth this out. It's also much less of an issue for datasets with many rows.\n",
    "\n",
    "\n",
    "\n",
    "* There is a function called partial_dependence to get the raw data making up this plot, rather than making the visual plot itself. This is useful if you want to control how it is visualized using a plotting package like Seaborn. With moderate effort, you could make much nicer looking plots."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Another Example: Titanic Dataest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-13T09:37:00.358416Z",
     "start_time": "2018-03-13T09:37:00.103533Z"
    },
    "collapsed": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAa4AAADPCAYAAACtKjXRAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x114a06588>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "titanic_data = pd.read_csv('Data/titanic.csv')\n",
    "titanic_y = titanic_data.Survived\n",
    "clf = GradientBoostingClassifier()\n",
    "titanic_X_colns = ['PassengerId','Age','Fare',]\n",
    "titanic_X = titanic_data[titanic_X_colns]\n",
    "my_imputer = Imputer()\n",
    "imputed_titanic_X = my_imputer.fit_transform(titanic_X)\n",
    "\n",
    "clf.fit(imputed_titanic_X, titanic_y)\n",
    "titanic_plots = plot_partial_dependence(clf, features=[1,2], X=imputed_titanic_X,\n",
    "                                       feature_names=titanic_X_colns, grid_resolution=8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "These might seem surprising at first glance. But they show some interesting insights:\n",
    "\n",
    "* Being young increased your odds of survival. This is consistent with historical recountings that they got women and children off the Titanic first.\n",
    "\n",
    "\n",
    "* People who paid more had better odds of survival. It turns out that higher fares got you a cabin that was closer to the top of the boat, and may have given you better odds of getting a life-boat."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# <center> L5: Pipelines\n",
    "\n",
    "Source: https://www.kaggle.com/dansbecker/pipelines\n",
    "\n",
    "Pipelines are a simple way to keep your data processing and modeling code organized. Specifically, a pipeline bundles preprocessing and modeling steps so you can use the whole bundle as if it were a single step.\n",
    "\n",
    "\n",
    "Many data scientists hack together models without pipelines, but Pipelines have some important benefits. Those include:\n",
    "\n",
    "* Cleaner Code: You won't need to keep track of your training (and validation) data at each step of processing. Accounting for data at each step of processing can get messy. With a pipeline, you don't need to manually keep track of each step.\n",
    "\n",
    "\n",
    "* Fewer Bugs: There are fewer opportunities to mis-apply a step or forget a pre-processing step.\n",
    "\n",
    "\n",
    "* Easier to Productionize: It can be surprisingly hard to transition a model from a prototype to something deployable at scale. We won't go into the many related concerns here, but pipelines can help.\n",
    "\n",
    "\n",
    "* More Options For Model Testing: You will see an example in the next tutorial, which covers cross-validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-13T09:37:00.430374Z",
     "start_time": "2018-03-13T09:37:00.359783Z"
    },
    "collapsed": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Read the data in and split it into training and testing group \n",
    "data = pd.read_csv('Data/melb_data.csv')\n",
    "cols_to_use = ['Rooms', 'Distance', 'Landsize', 'BuildingArea', 'YearBuilt']\n",
    "X = data[cols_to_use]\n",
    "y = data.Price\n",
    "train_X, test_X, train_y, test_y = train_test_split(X,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-13T09:37:00.435721Z",
     "start_time": "2018-03-13T09:37:00.431798Z"
    },
    "collapsed": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# Import the stages of the DataFrame\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import Imputer\n",
    "\n",
    "# Creating the Pipeline\n",
    "my_pipeline = make_pipeline(Imputer(), RandomForestRegressor())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-13T09:37:00.699060Z",
     "start_time": "2018-03-13T09:37:00.437115Z"
    },
    "collapsed": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 2244200.        ,  1055998.21587726,   830100.        , ...,\n",
       "         837500.        ,  1396800.        ,  1954600.        ])"
      ]
     },
     "execution_count": 158,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Fit the datframe to your pipeline\n",
    "my_pipeline.fit(train_X, train_y)\n",
    "\n",
    "# View the predictions from the dataframe\n",
    "predictions = my_pipeline.predict(test_X)\n",
    "predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "\n",
    "# <center> L6: Cross Validation\n",
    "\n",
    "Source: https://www.kaggle.com/dansbecker/cross-validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "\n",
    "### Trade-offs\n",
    "\n",
    "* Cross-validation gives a more accurate measure of model quality, which is especially important if you are making a lot of modeling decisions. However, it can take more time to run, because it estimates models once for each fold. So it is doing more total work.\n",
    "\n",
    "\n",
    "* For the same reasons, a simple train-test split is sufficient for larger datasets. It will run faster, and you may have enough data that there's little need to re-use some of it for holdout.\n",
    "\n",
    "\n",
    "* There's no simple threshold for what constitutes a large vs small dataset. If your model takes a couple minute or less to run, it's probably worth switching to cross-validation. If your model takes much longer to run, cross-validation may slow down your workflow more than it's worth."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-13T09:37:00.781792Z",
     "start_time": "2018-03-13T09:37:00.700471Z"
    },
    "collapsed": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "import pandas as pd\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import Imputer\n",
    "\n",
    "#  Import trainin and testing datasets\n",
    "data = pd.read_csv('Data/melb_data.csv')\n",
    "cols_to_use = ['Rooms', 'Distance', 'Landsize', 'BuildingArea', 'YearBuilt']\n",
    "X = data[cols_to_use]\n",
    "y = data.Price\n",
    "\n",
    "# Create a pipeline\n",
    "my_pipeline = make_pipeline(Imputer(), RandomForestRegressor())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-13T09:37:01.474190Z",
     "start_time": "2018-03-13T09:37:00.783257Z"
    },
    "collapsed": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-324070.99804702 -300938.71715819 -279986.25416915]\n"
     ]
    }
   ],
   "source": [
    "# Sklearn 'cross_val_score metric' can give a cross_valitated score for any dataset\n",
    "from sklearn.model_selection import cross_val_score\n",
    "scores = cross_val_score(my_pipeline, X, y, scoring='neg_mean_absolute_error')\n",
    "print(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-13T09:37:01.479405Z",
     "start_time": "2018-03-13T09:37:01.475837Z"
    },
    "collapsed": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Absolute Error 301665.323125\n"
     ]
    }
   ],
   "source": [
    "# Taking the mean of the scores, as the best performance measure\n",
    "print('Mean Absolute Error %2f' %(-1 * scores.mean()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-14T01:41:11.974624Z",
     "start_time": "2018-02-14T01:41:11.972368Z"
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# <center> L7: Data Leakage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "Source: https://www.kaggle.com/dansbecker/data-leakage\n",
    "### What is Data Leakage\n",
    "\n",
    "* Leakage: specifically, leakage causes a model to look accurate until you start making decisions with the model, and then the model becomes very inaccurate\n",
    "\n",
    "### Leaky Predictors\n",
    "\n",
    "* This occurs when your predictors include data that will not be available at the time you make predictions.\n",
    "\n",
    "\n",
    "* People take antibiotic medicines after getting pneumonia in order to recover. So the raw data shows a strong relationship between those columns. But took_antibiotic_medicine is frequently changed after the value for got_pneumonia is determined. This is target leakage. The model would see that anyone who has a value of False for took_antibiotic_medicine didn't have pneumonia. Validation data comes from the same source, so the pattern will repeat itself in validation, and the model will have great validation (or cross-validation) scores. But the model will be very inaccurate when subsequently deployed in the real world. To prevent this type of data leakage, any variable updated (or created) after the target value is realized should be excluded. Because when we use this model to make new predictions, that data won't be available to the model.\n",
    "\n",
    "### Leaky Validation Strategy\n",
    "\n",
    "*  much different type of leak occurs when you aren't careful distinguishing training data from validation data. For example, this happens if you run preprocessing (like fitting the Imputer for missing values) before calling train_test_split. Validation is meant to be a measure of how the model does on data it hasn't considered before. You can corrupt this process in subtle ways if the validation data affects the preprocessing behavoir.. The end result? Your model will get very good validation scores, giving you great confidence in it, but perform poorly when you deploy it to make decisions.\n",
    "\n",
    "### Preventing Leaky Predictors\n",
    "* However, leaky predictors frequently have high statistical correlations to the target. So two tactics to keep in mind:\n",
    "\n",
    "    - To screen for possible leaky predictors, look for columns that are statistically correlated to your target.\n",
    "    - If you build a model and find it extremely accurate, you likely have a leakage problem.\n",
    "    \n",
    "### Preventing Leaky Validation Strategies\n",
    "* If your validation is based on a simple train-test split, exclude the validation data from any type of fitting, including the fitting of preprocessing steps. This is easier if you use scikit-learn Pipelines. When using cross-validation, it's even more critical that you use pipelines and do your preprocessing inside the pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-13T09:37:01.500911Z",
     "start_time": "2018-03-13T09:37:01.481403Z"
    },
    "collapsed": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>card</th>\n",
       "      <th>reports</th>\n",
       "      <th>age</th>\n",
       "      <th>income</th>\n",
       "      <th>share</th>\n",
       "      <th>expenditure</th>\n",
       "      <th>owner</th>\n",
       "      <th>selfemp</th>\n",
       "      <th>dependents</th>\n",
       "      <th>months</th>\n",
       "      <th>majorcards</th>\n",
       "      <th>active</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>True</td>\n",
       "      <td>0</td>\n",
       "      <td>37.66667</td>\n",
       "      <td>4.5200</td>\n",
       "      <td>0.033270</td>\n",
       "      <td>124.983300</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>3</td>\n",
       "      <td>54</td>\n",
       "      <td>1</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>True</td>\n",
       "      <td>0</td>\n",
       "      <td>33.25000</td>\n",
       "      <td>2.4200</td>\n",
       "      <td>0.005217</td>\n",
       "      <td>9.854167</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>3</td>\n",
       "      <td>34</td>\n",
       "      <td>1</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>True</td>\n",
       "      <td>0</td>\n",
       "      <td>33.66667</td>\n",
       "      <td>4.5000</td>\n",
       "      <td>0.004156</td>\n",
       "      <td>15.000000</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>4</td>\n",
       "      <td>58</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>True</td>\n",
       "      <td>0</td>\n",
       "      <td>30.50000</td>\n",
       "      <td>2.5400</td>\n",
       "      <td>0.065214</td>\n",
       "      <td>137.869200</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "      <td>25</td>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>True</td>\n",
       "      <td>0</td>\n",
       "      <td>32.16667</td>\n",
       "      <td>9.7867</td>\n",
       "      <td>0.067051</td>\n",
       "      <td>546.503300</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>2</td>\n",
       "      <td>64</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   card  reports       age  income     share  expenditure  owner  selfemp  \\\n",
       "0  True        0  37.66667  4.5200  0.033270   124.983300   True    False   \n",
       "1  True        0  33.25000  2.4200  0.005217     9.854167  False    False   \n",
       "2  True        0  33.66667  4.5000  0.004156    15.000000   True    False   \n",
       "3  True        0  30.50000  2.5400  0.065214   137.869200  False    False   \n",
       "4  True        0  32.16667  9.7867  0.067051   546.503300   True    False   \n",
       "\n",
       "   dependents  months  majorcards  active  \n",
       "0           3      54           1      12  \n",
       "1           3      34           1      13  \n",
       "2           4      58           1       5  \n",
       "3           0      25           1       7  \n",
       "4           2      64           1       5  "
      ]
     },
     "execution_count": 162,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Import credit card data\n",
    "data = pd.read_csv('Data/AER_credit_card_data.csv', \n",
    "                   true_values = ['yes'],\n",
    "                   false_values = ['no'])\n",
    "data.head(n=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-13T09:37:01.562665Z",
     "start_time": "2018-03-13T09:37:01.502125Z"
    },
    "collapsed": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross-val accuracy: 0.980284\n"
     ]
    }
   ],
   "source": [
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "# Set target and predictors\n",
    "y = data.card\n",
    "X = data.drop(['card'], axis=1)\n",
    "\n",
    "# Since there was no preprocessing, we didn't need a pipeline here.\n",
    "# Used anyway as best practice.\n",
    "\n",
    "# Creating a model Pipeline\n",
    "modeling_pipeline = make_pipeline(RandomForestClassifier())\n",
    "cv_scores = cross_val_score(modeling_pipeline, X, y, scoring='accuracy')\n",
    "print(\"Cross-val accuracy: %f\" %cv_scores.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-13T09:37:01.571460Z",
     "start_time": "2018-03-13T09:37:01.563966Z"
    },
    "code_folding": [
     3
    ],
    "collapsed": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fraction of those who received a card with no expenditures: 0.02\n",
      "Fraction of those who received a card with no expenditures: 1.00\n"
     ]
    }
   ],
   "source": [
    "# Grouping expenditure data based on card and non-card holder\n",
    "expenditures_cardholders = data.expenditure[data.card]\n",
    "expenditures_noncardholders = data.expenditure[~data.card]\n",
    "\n",
    "# Printing Mean for card expentiture grouping\n",
    "print('Fraction of those who received a card with no expenditures: %.2f' \\\n",
    "      %(( expenditures_cardholders == 0).mean()))\n",
    "print('Fraction of those who received a card with no expenditures: %.2f' \\\n",
    "      %((expenditures_noncardholders == 0).mean()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-13T09:37:01.637408Z",
     "start_time": "2018-03-13T09:37:01.573501Z"
    },
    "collapsed": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross-val accuracy: 0.803641\n"
     ]
    }
   ],
   "source": [
    "# Author identifies features that may be causing leaks\n",
    "potential_leaks = ['expenditure', 'share', 'active', 'majorcards']\n",
    "\n",
    "# Drop leaky features from dataframe\n",
    "X2 = X.drop(potential_leaks, axis=1)\n",
    "\n",
    "# computed cross_val_scores for our pipeline\n",
    "cv_scores = cross_val_score(modeling_pipeline, X2, y, scoring='accuracy')\n",
    "\n",
    "# print the mean cross_val_score\n",
    "print(\"Cross-val accuracy: %f\" %cv_scores.mean())"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
