{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Learning with Python: Deep Learing from Text and Sequences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Work-level one-hot encoding (toy example)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "samples = ['The cat sat on the mat.', 'The dog ate my homework.']\n",
    "\n",
    "token_index = {}\n",
    "for sample in samples:\n",
    "    for word in sample.split():\n",
    "        if word not in token_index:\n",
    "            token_index[word] = len(token_index) + 1\n",
    "\n",
    "max_length = 10\n",
    "\n",
    "results = np.zeros(shape=(len(samples),\n",
    "                  max_length, \n",
    "                  max(token_index.values()) +1 ))\n",
    "\n",
    "for i, sample in enumerate(samples):\n",
    "    for j, word in list(enumerate(sample.split()))[:max_length]:\n",
    "        index = token_index.get(word)\n",
    "        results[i,j, index] = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using Keras for word-level one-hot encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 9 unique tokens.\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "samples = ['The cat sat on the mat.', 'The dog ate my homework.']\n",
    "\n",
    "# Creat a tokenizer that only includes the top 100 words\n",
    "tokenizer = Tokenizer(num_words=100)\n",
    "# Build the word index\n",
    "tokenizer.fit_on_texts(samples)\n",
    "\n",
    "# Turn stings into lists of integer indices\n",
    "seuqences = tokenizer.texts_to_sequences(samples)\n",
    "\n",
    "one_hit_results = tokenizer.texts_to_matrix(samples, mode='binary')\n",
    "\n",
    "word_index = tokenizer.word_index\n",
    "print('Found %s unique tokens.' %len(word_index))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word-Level One-Hot Encoding with hashing trick"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-3-013f8edb3a9f>, line 7)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-3-013f8edb3a9f>\"\u001b[0;36m, line \u001b[0;32m7\u001b[0m\n\u001b[0;31m    fro j, word in list(enumerate(sample.split()))[:max_length]:\u001b[0m\n\u001b[0m        ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "samples = ['The cat sat on the mat.', 'The dog ate my homework.']\n",
    "dimensionality = 1000\n",
    "max_lenght = 10\n",
    "\n",
    "results = np.zeros((len(samples), max_length, dimensionality))\n",
    "for i, sample in enumerat(samples):\n",
    "    fro j, word in list(enumerate(sample.split()))[:max_length]:\n",
    "        # Hashes the word into a random integer index between 0 and 1,000\n",
    "        index = abs(hash(word)) % dimensionality\n",
    "        results[i, j, index]  = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Instantiating and Embedding Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Embedding\n",
    "embedding_layer = Embedding(1000,4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading the IMDB dat afor use with an Embedding Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.datasets import imdb\n",
    "from keras import preprocessing\n",
    "\n",
    "max_features = 10000 #Number of words to consider as features\n",
    "# Cuts off the text after this number of words \n",
    "# (among the max_features most common words)\n",
    "maxlen = 20 \n",
    "\n",
    "# Load the data as a lists of intergers\n",
    "(x_train, y_train), (x_test, y_test) = imdb.load_data(\n",
    "    num_words=max_features)\n",
    "\n",
    "# Turns the lists of integers into a 2D integer tensor of shapes\n",
    "x_train = preprocessing.sequence.pad_sequences(x_train, maxlen=maxlen)\n",
    "x_test = preprocessing.sequence.pad_sequences(x_test, maxlen=maxlen)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ues an Embedding layer and classifier on the IMDB data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Flatten, Dense\n",
    "\n",
    "# model = Sequetial()\n",
    "model = Sequential()\n",
    "model.add(Embedding(10000, 8, input_length=maxlen))\n",
    "\n",
    "# Flattens the 3D tensor of embeddings into a 2D tensor of shape (samples, maxlen * 8)\n",
    "model.add(Flatten())\n",
    "\n",
    "# Adds the classifier on top\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "model.compile(optimizer='rmsprop', loss='binary_crossentropy', \n",
    "              metrics=['acc'])\n",
    "model.summary()\n",
    "\n",
    "history = model.fit(x_train, y_train, epochs=10, batch_size=12, \n",
    "                   validation_split=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You get to a validation accuracy of ~76%, which is pretty good considering that you’re only looking at the first 20 words in every review. But note that merely flattening the embedded sequences and training a single Dense layer on top leads to a model that treats each word in the input sequence separately, without considering inter-word relationships and sentence structure (for example, this model would likely treat both “this movie is a bomb” and “this movie is the bomb” as being negative reviews). It’s much better to add recurrent layers or 1D convolutional layers on top of the embed- ded sequences to learn features that take into account each sequence as a whole. That’s what we’ll focus on in the next few sections."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Putting it all together: from raw text to word embeddings\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You’ll use a model similar to the one we just went over: embedding sentences in sequences of vectors, flattening them, and training a Dense layer on top. But you’ll do so using pretrained word embeddings; and instead of using the pretokenized IMDB data packaged in Keras, you’ll start from scratch by downloading the original text data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "imdb_dir = '/Users/Kavi/Dropbox/DataScience/Tutorials/Tutorial - Keras/Data/aclImdb'\n",
    "train_dir = os.path.join(imdb_dir, 'train')\n",
    "\n",
    "labels = []\n",
    "texts = []\n",
    "\n",
    "for label_type in ['neg','pos']:\n",
    "    dir_name = os.path.join(train_dir, label_type)\n",
    "    for fname in os.listdir(dir_name):\n",
    "        f = open(os.path.join(dir_name, fname))\n",
    "        texts.append(f.read())\n",
    "        f.close()\n",
    "        if label_type == 'neg':\n",
    "            labels.append(0)\n",
    "        else:\n",
    "            labels.append(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dir_name = os.path.join(train_dir, label_type)\n",
    "dir_name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenizing the text of the raw IMDB data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let’s vectorize the text and prepare a training and validation split, using the concepts introduced earlier in this section. Because pretrained word embeddings are meant to be particularly useful on problems where little training data is available (otherwise, task-specific embeddings are likely to outperform them), we’ll add the following twist: restricting the training data to the first 200 samples. So you’ll learn to classify movie reviews after looking at just 200 examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "import numpy as np\n",
    "maxlen=100\n",
    "training_samples=10000\n",
    "validation_samples = 200\n",
    "max_words=10000\n",
    "\n",
    "tokenizer = Tokenizer(num_words=max_words)\n",
    "tokenizer.fit_on_texts(texts)\n",
    "sequences = tokenizer.texts_to_sequences(texts)\n",
    "\n",
    "word_index = tokenizer.word_index\n",
    "print('Found %s unique tokens.' %len(word_index))\n",
    "\n",
    "data = pad_sequences(sequences, maxlen=maxlen)\n",
    "\n",
    "labels = np.asarray(labels)\n",
    "print('Shape of data tensor:', data.shape)\n",
    "print('Shape of label tensor:', labels.shape)\n",
    "\n",
    "# Splits the data into a training set and a validation set, \n",
    "# but first shuffles the data, because you’re starting with data in which samples are ordered (all negative first, then all positive)\n",
    "indices = np.arange(data.shape[0])\n",
    "np.random.shuffle(indices)\n",
    "data = data[indices]\n",
    "\n",
    "labels = labels[indices]\n",
    "\n",
    "x_train = data[:training_samples]\n",
    "y_train = labels[:training_samples]\n",
    "x_val = data[training_samples: training_samples + validation_samples]\n",
    "y_val = labels[\n",
    "training_samples: training_samples + validation_samples]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parsing the GloVe work-embedding file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Preparing the GloVe word-embeddings matrix\n",
    "embedding_dim = 100\n",
    "embedding_matrix = np.zeros((max_words, embedding_dim))\n",
    "for word, i in word_index.items():\n",
    "    if i < max_words:\n",
    "        embedding_vector = embeddings_index.get(word)\n",
    "        if embedding_vector is not None:\n",
    "            embedding_matrix[i] = embedding_vector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Embedding, Flatten, Dense\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(max_words, embedding_dim, input_length=maxlen))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(32, activation='relu'))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading Pretrained Word Embedding into the Embedding Layer\n",
    "The Embedding layer has a single weight matrix: a 2D float matrix where each entry i is the word vector meant to be associated with index i. Simple enough. Load the GloVe matrix you prepared into the Embedding layer, the first layer in the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.layers[0].set_weights([embedding_matrix])\n",
    "model.layers[0].trainable = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "dditionally, you’ll freeze the Embedding layer (set its trainable attribute to False), following the same rationale you’re already familiar with in the context of pretrained convnet features: when parts of a model are pretrained (like your Embedding layer) and parts are randomly initialized (like your classifier), the pretrained parts shouldn’t be updated during training, to avoid forgetting what they already know. The large gra- dient updates triggered by the randomly initialized layers would be disruptive to the already-learned features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training and Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='rmsprop', loss='binary_crossentropy',\n",
    "             metrics=['acc'])\n",
    "histroy  = model.fit(x_train, y_train, epochs=10, batch_size=32,\n",
    "                     validation_data=(x_val, y_val))\n",
    "model.save_weights('pre_trained_glove_model.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Plot the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "acc = history.history['acc']\n",
    "val_acc = history.history['val_acc']\n",
    "loss = history.history['loss']\n",
    "val_loss = history.history['val_loss']\n",
    "\n",
    "epochs = range(1, len(acc) + 1)\n",
    "\n",
    "plt.plot(epochs, acc, 'bo', label='Training acc')\n",
    "plt.plot(epochs, val_acc, 'b', label='Validation acc')\n",
    "plt.title('Training and validation accuracy')\n",
    "plt.legend()\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(epochs, loss, 'bo', label='Training loss')\n",
    "plt.plot(epochs, val_loss, 'b', label='Validation loss')\n",
    "plt.title('Training and validation loss')\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, let’s evaluate the model on the test data. First, you need to tokenize the test data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenizing the data of the test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dir = os.path.join(imdb_dir, 'test')\n",
    "labels = []\n",
    "texts = []\n",
    "for label_type in ['neg','pos']:\n",
    "    dir_name = os.path.join(test_dir, label_type)\n",
    "    for fname in sorted(os.listdir(dir_name)):\n",
    "        if fname[-4:] == '.txt':\n",
    "            f = open(os.path.join(dir_name, fname))\n",
    "            texts.append(f.read())\n",
    "            f.close()\n",
    "            if label_type == 'neg':\n",
    "                labels.append(0)\n",
    "            else:\n",
    "                labels.append(1)\n",
    "sequences = tokenizer.texts_to_sequences(texts)\n",
    "x_test = pad_sequences(sequences, maxlen=maxlen)\n",
    "y_test = np.array(labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluating the model on the test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_weights('pre_trained_glove_model.h5')\n",
    "model.evaluate(x_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Understanding Recurrent Neural Networks\n",
    "\n",
    "To make these notions of loop and state clear, let’s implement the forward pass of a toy RNN in Numpy. This RNN takes as input a sequence of vectors, which you’ll encode as a 2D tensor of size (timesteps, input_features). It loops over timesteps, and at each timestep, it considers its current state at t and the input at t (of shape (input_ features,), and combines them to obtain the output at t. You’ll then set the state for the next step to be this previous output. For the first timestep, the previous output isn’t defined; hence, there is no current state. So, you’ll initialize the state as an all- zero vector called the initial state of the network."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pseudocode RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#state_t = 0\n",
    "#for input_t in input_sequence:\n",
    "#    output_t = f(input_t, state_t)\n",
    "#    state_t = output_t"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Numpy Implementation of a simple RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "timesteps = 100\n",
    "input_features =32\n",
    "output_features = 64\n",
    "\n",
    "input = np.random.random((timesteps, input_features))\n",
    "state_t = np.zeros((output_features,))\n",
    "\n",
    "W = np.random.random((output_features, input_features))\n",
    "U = np.random.random((output_features, output_features))\n",
    "b = np.random.random((output_features,))\n",
    "\n",
    "successive_outputs = []\n",
    "for input_t in inputs:\n",
    "    output_t = np.tanh(np.dot(W, input_t) + np.dot(U, state_t) + b)\n",
    "    successive_outputs.append(output)t\n",
    "    state_t = output_t\n",
    "final_output_sequence = np.concatenate(successive_outputs, axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A recurrent layer in Keras\n",
    "he process you just naively implemented in Numpy corresponds to an actual Keras layer—the SimpleRNN layer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import SimpleRNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Like all recurrent layers in Keras, SimpleRNN can be run in two different modes: it can return either the full sequences of successive outputs for each timestep (a 3D ten- sor of shape (batch_size, timesteps, output_features)) or only the last output for each input sequence (a 2D tensor of shape (batch_size, output_features)). These two modes are controlled by the return_sequences constructor argument. Let’s look at an example that uses SimpleRNN and returns only the output at the last timestep:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Embedding, SimpleRNN\n",
    "model = Sequential()\n",
    "model.add(Embedding(1000, 32))\n",
    "model.add(SimpleRNN(32))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following example returns the full state sequence:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Embedding(10000, 32))\n",
    "model.add(SimpleRNN(32, return_sequences=True))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "It’s sometimes useful to stack several recurrent layers one after the other in order to increase the representational power of a network. In such a setup, you have to get all of the intermediate layers to return full sequence of outputs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Embedding(10000, 32))\n",
    "model.add(SimpleRNN(32, return_sequences=True))\n",
    "model.add(SimpleRNN(32, return_sequences=True))\n",
    "model.add(SimpleRNN(32, return_sequences=True))\n",
    "model.add(SimpleRNN(32))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let’s use such a model on the IMDB movie-review-classification problem. First,\n",
    "preprocess the data.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preparing the IMDB datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.datasets import imdb\n",
    "from keras.preprocessing import sequence\n",
    "\n",
    "max_features = 10000\n",
    "maxlen =  500\n",
    "batch_size = 32\n",
    "\n",
    "print('Loading data..')\n",
    "(input_train, y_train), (input_test, y_test) = imdb.load_data(\n",
    "    num_words=max_features)\n",
    "print(len(input_train), 'train sequence')\n",
    "print(len(input_test), 'test sequence')\n",
    "\n",
    "print('Pad sequence (samples x time)')\n",
    "input_train = sequence.pad_sequences(input_train, maxlen=maxlen)\n",
    "input_train = sequence.pad_sequences(input_test, maxlen=maxlen)\n",
    "print('input_train.shape:', input_train.shape)\n",
    "print('input_test.shape:',input_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training the model with Embedding and Simple RNN layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Dense\n",
    "model =  Sequential()\n",
    "model.add(Embedding(max_features, 32))\n",
    "model.add(SimpleRNN(32))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "model.compile(optimizer='rmsprop', loss='binary_crossentropy', metrics=['acc'])\n",
    "history = model.fit(input_train, y_train, epochs=10,\n",
    "                   batch_size=128, validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "acc = history.history['acc']\n",
    "val_acc = history.history['val_acc']\n",
    "loss = history.history['loss']\n",
    "val_loss = history.history['val_loss']\n",
    "epochs = range(1, len(acc) + 1)\n",
    "plt.plot(epochs, acc, 'bo', label='Training acc')\n",
    "plt.plot(epochs, val_acc, 'b', label='Validation acc')\n",
    "plt.title('Training and validation accuracy')\n",
    "plt.legend()\n",
    "plt.figure()\n",
    "plt.plot(epochs, loss, 'bo', label='Training loss')\n",
    "plt.plot(epochs, val_loss, 'b', label='Validation loss')\n",
    "plt.title('Training and validation loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using the LSTM layer in Keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import LSTM\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(max_features, 32))\n",
    "model.add(LSTM(32))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "model.compile(optimizer='rmsprop', loss='binary_crossentropy',\n",
    "             metrics=['acc'])\n",
    "history = model.fit(input_train, y_train, epochs=10, batch_size=128,\n",
    "                   validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fix: Creating a plotting Function\n",
    "def plot_history(history):\n",
    "    acc = history.history['acc']\n",
    "    val_acc = history.history['val_acc']\n",
    "    loss = history.history['loss']\n",
    "    val_loss = history.history['val_loss']\n",
    "    epochs = range(1, len(acc) + 1)\n",
    "    plt.plot(epochs, acc, 'bo', label='Training acc')\n",
    "    plt.plot(epochs, val_acc, 'b', label='Validation acc')\n",
    "    plt.title('Training and validation accuracy')\n",
    "    plt.legend()\n",
    "    plt.figure()\n",
    "    plt.plot(epochs, loss, 'bo', label='Training loss')\n",
    "    plt.plot(epochs, val_loss, 'b', label='Validation loss')\n",
    "    plt.title('Training and validation loss')\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_history(history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This time, you achieve up to 89% validation accuracy. Not bad: certainly much better than the SimpleRNN network—that’s largely because LSTM suffers much less from the vanishing-gradient problem—and slightly better than the fully connected approach from chapter 3, even though you’re looking at less data than you were in chapter 3. You’re truncating sequences after 500 timesteps, whereas in chapter 3, you were con- sidering full sequences."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Advanced use of recurrent neural networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inspecting the data fo the Jena Weather Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['\"Date Time\"', '\"p (mbar)\"', '\"T (degC)\"', '\"Tpot (K)\"', '\"Tdew (degC)\"', '\"rh (%)\"', '\"VPmax (mbar)\"', '\"VPact (mbar)\"', '\"VPdef (mbar)\"', '\"sh (g/kg)\"', '\"H2OC (mmol/mol)\"', '\"rho (g/m**3)\"', '\"wv (m/s)\"', '\"max. wv (m/s)\"', '\"wd (deg)\"']\n",
      "420551\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "data_dir = '/Users/Kavi/Dropbox/DataScience/Tutorials/Tutorial - Keras/Data'\n",
    "fname = os.path.join(data_dir, 'jena_climate_2009_2016.csv')\n",
    "\n",
    "f = open(fname)\n",
    "data = f.read()\n",
    "f.close()\n",
    "\n",
    "lines = data.split('\\n')\n",
    "header = lines[0].split(',')\n",
    "lines = lines[1:]\n",
    "\n",
    "print(header)\n",
    "print(len(lines))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parsing the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "float_data = np.zeros((len(lines), len(header) -1))\n",
    "for i, line in enumerate(lines):\n",
    "    values = [float(x) for x in line.split(',')[1:]]\n",
    "    float_data[i,:] = values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plotting the temperature timeseries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x12d0f4518>]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXwAAAD8CAYAAAB0IB+mAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x127373780>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline \n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "temp = float_data[:,1]\n",
    "plt.plot(range(len(temp)), temp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plotting the first 10 days of the temperature timeseries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x12d1785f8>]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXwAAAD8CAYAAAB0IB+mAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nJPgEQRBFAgk+QRBEkfD/ATyGJJoJE7qrAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x11d02e4e0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(range(1440), temp[:1440])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Forcasting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you were trying to predict average temperature for the next month given a few months of past data, the problem would be easy, due to the reliable year-scale period- icity of the data. But looking at the data over a scale of days, the temperature looks a lot more chaotic. Is this timeseries predictable at a daily scale? Let’s find out."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normalizing the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  988.88635885,     9.07734895,   283.14631345,     4.44854725,\n",
       "          75.35405895,    13.3829553 ,     9.29695535,     4.08591725,\n",
       "           5.8752111 ,     9.40521005,  1217.51429655,     2.1510037 ,\n",
       "           3.56950915,   176.21703355])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mean = float_data[:200000].mean(axis=0)\n",
    "mean"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generating Yielding timeseries samples and their targetm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generator(data, lookback, delay, min_index, max_index,\n",
    "             shuffle:False, batch_size=128, step=6):\n",
    "    \n",
    "    if max_index is None:\n",
    "        max_index = len(data) - delay - 1\n",
    "    i = min_index + lookback\n",
    "    while 1:\n",
    "        if shuffle:\n",
    "            rows = np.random.randint(min_index + lookback, max_index, size=batch_size)\n",
    "        else:\n",
    "            if i + batch_size >= max_index:\n",
    "                i = min_index + lookback\n",
    "            rows = np.arange(i, min(i + batch_size, max_index))\n",
    "            i += len(rows)\n",
    "        samples = np.zeros((len(rows), lookback // step, data.shape[-1]))\n",
    "        targets = np.zeros((len(rows),))\n",
    "        for j, row in enumerate(rows):\n",
    "            indices = range(rows[j] - lookback, rows[j], step)\n",
    "            samples[j] = data[indices]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Preparing the training, validation, and test generotrs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lookback = 1440\n",
    "step = 6\n",
    "delay = 144\n",
    "batch_size =128\n",
    "\n",
    "\n",
    "train_gen = generator(float_data, lookback=lookback, delay=delay, min_index=0,\n",
    "                      max_index=200000., shuffle=True, step=step,\n",
    "                      batch_size = batch_size)\n",
    "val_gen = generator(float_data, lookback=lookback, delay=delay, min_index=200001,\n",
    "                      max_index=300000., shuffle=True, step=step,\n",
    "                      batch_size = batch_size)\n",
    "test_gen = generator(float_data, lookback=lookback, delay=delay, min_index=300001,\n",
    "                      max_index=None, shuffle=True, step=step,\n",
    "                      batch_size = batch_size)\n",
    "\n",
    "val_step = (300000 - 200001 - lookback)\n",
    "test_step = (len(float_data) - 300001 - lookback)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A commen sense, non machine learning baseline\n",
    "\n",
    "Before you start using black-box deep-learning models to solve the temperature- prediction problem, let’s try a simple, common-sense approach. It will serve as a sanity check, and it will establish a baseline that you’ll have to beat in order to demonstrate the usefulness of more-advanced machine-learning models. Such common-sense base- lines can be useful when you’re approaching a new problem for which there is no known solution (yet). A classic example is that of unbalanced classification tasks, where some classes are much more common than others. If your dataset contains 90% instances of class A and 10% instances of class B, then a common-sense approach to the classification task is to always predict “A” when presented with a new sample. Such a classifier is 90% accurate overall, and any learning-based approach should therefore beat this 90% score in order to demonstrate usefulness. Sometimes, such elementary baselines can prove surprisingly hard to beat.\n",
    "\n",
    "In this case, the temperature timeseries can safely be assumed to be continuous (the temperatures tomorrow are likely to be close to the temperatures today) as well as periodical with a daily period. Thus a common-sense approach is to always predict that the temperature 24 hours from now will be equal to the temperature right now. Let’s evaluate this approach, using the mean absolute error (MAE) metric:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#np.mean(np.abs(preds - targets))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Computing the common-sense baseline MAE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_naive_method():\n",
    "    batch_maes = []\n",
    "    for step in range(val_steps):\n",
    "        samples, targets = next(val_gen)\n",
    "        preds =  sample(np.abs(preds - targets))\n",
    "        batch_maes.append(mae)\n",
    "    print(np.mean(batch_meas))\n",
    "    \n",
    "evaluate_naive_method()\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This yields an MAE of 0.29. Because the temperature data has been normalized to be centered on 0 and have a standard deviation of 1, this number isn’t immediately inter- pretable. It translates to an average absolute error of 0.29 × temperature_std degrees Celsius: 2.57 ̊C."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Converting the MAE back to a Celsuis error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "celsius_mae = 0.29 * std[1]\n",
    "celsius_mae"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "That’s a fairly large average absolute error. Now the game is to use your knowledge of deep learning to do better."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training and Evaluating a densely connected Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " \n",
    "In the same way that it’s useful to establish a common-sense baseline before trying machine-learning approaches, it’s useful to try simple, cheap machine-learning mod- els (such as small, densely connected networks) before looking into complicated and computationally expensive models such as RNNs. This is the best way to make sure any further complexity you throw at the problem is legitimate and delivers real benefits.\n",
    "\n",
    "\n",
    "The following listing shows a fully connected model that starts by flattening the data and then runs it through two Dense layers. Note the lack of activation function on the last Dense layer, which is typical for a regression problem. You use MAE as the loss. Because you evaluate on the exact same data and with the exact same metric you did with the common-sense approach, the results will be directly comparable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training and Evaluating a Densely Connected Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras import layers\n",
    "from keras.optimizers import RMSprop\n",
    "\n",
    "model = Sequential()\n",
    "model.add(layers.Flatten(input_shape=(lookback // step, float_data.shape[-1])))\n",
    "model.add(layers.Dense(32, activation='relu'))\n",
    "model.add(layers.Dense[1])\n",
    "\n",
    "model.compile(optimizer=RMsprop(), loss='mae')\n",
    "history = model.fit_generator(train_gen, steps_per_epoch=500,\n",
    "                             epochs=20, validation_data=val_gen,\n",
    "                             validation_step=val_steps)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plotting Result "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_histor(history):\n",
    "    loss = history.history('loss')\n",
    "    val_loss = history.history['val_loss']\n",
    "\n",
    "    epochs = range(1, len(loss) +1)\n",
    "\n",
    "    plt.figure()\n",
    "\n",
    "    plt.plot(epochs, loss, 'bo', label='Training loss')\n",
    "    plt.plot(epochs, val_loss, 'b', label='Validation loss')\n",
    "    plt.title('Trainig and Validation loss')\n",
    "    plt.show()\n",
    "\n",
    "plot_history(history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Triaining and Evaluating a GRU-based model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.model import Sequential\n",
    "from keras import layers\n",
    "from keras.optimizers import RMSprop\n",
    "\n",
    "model = Sequetial()\n",
    "model.add(layers.GRU(32, input_shape=(None, float_data.shape[-1])))\n",
    "model.add(layers.Dense(1))\n",
    "model.compile(optimizer=RMSprop(), loss='mae')\n",
    "history.compile(optimizer =RMSprop(), loss='mae')\n",
    "history = model.fit_generator(train_gen, steps_per_epoch=500,\n",
    "                             epoch=20, validation_data=val_gen, \n",
    "                             validation_steps=val_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_history(history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The new validation MAE of ~0.265 (before you start significantly overfitting) translates to a mean absolute error of 2.35 ̊C after denormalization. That’s a solid gain on the initial error of 2.57 ̊C, but you probably still have a bit of a margin for improvement.`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using recurrent dropout to fight overfitting\n",
    "\n",
    "It’s evident from the training and validation curves that the model is overfitting: the training and validation losses start to diverge considerably after a few epochs. You’re already familiar with a classic technique for fighting this phenomenon: dropout, which randomly zeros out input units of a layer in order to break happenstance cor- relations in the training data that the layer is exposed to. But how to correctly apply dropout in recurrent networks isn’t a trivial question. It has long been known that applying dropout before a recurrent layer hinders learning rather than helping with regularization. In 2015, Yarin Gal, as part of his PhD thesis on Bayesian deep learn- ing,6 determined the proper way to use dropout with a recurrent network: the same dropout mask (the same pattern of dropped units) should be applied at every time- step, instead of a dropout mask that varies randomly from timestep to timestep. What’s more, in order to regularize the representations formed by the recurrent gates of layers such as GRU and LSTM, a temporally constant dropout mask should be applied to the inner recurrent activations of the layer (a recurrent dropout mask). Using the same dropout mask at every timestep allows the network to properly propagate its learning error through time; a temporally random dropout mask would disrupt this error signal and be harmful to the learning process.\n",
    "\n",
    "\n",
    "Yarin Gal did his research using Keras and helped build this mechanism directly into Keras recurrent layers. Every recurrent layer in Keras has two dropout-related arguments: dropout, a float specifying the dropout rate for input units of the layer,\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training and evaluating a dropout-regularized GRU-based model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras import layers\n",
    "from keras.optimizers improt RMSprop\n",
    "\n",
    "model = Sequential()\n",
    "model.add(layers.GRU(32, dropout=0.2, \n",
    "                     input_shape=(None, float_data.shape[-1])))\n",
    "model.compile(optimizer=RMSprpo(), loss='mae')\n",
    "history = model.fit_generator(train_gen, steps_per_epoch=500, \n",
    "                             epochs=40, validation_data=val_gen, \n",
    "                             validation_steps=val_steps)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Figure 6.22 shows the results. Success! You’re no longer overfitting during the first 30 epochs. But although you have more stable evaluation scores, your best scores aren’t much lower than they were previously."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stacking Recurrent Layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Because you’re no longer overfitting but seem to have hit a performance bottleneck, you should consider increasing the capacity of the network. Recall the description of the universal machine-learning workflow: it’s generally a good idea to increase the capacity of your network until overfitting becomes the primary obstacle (assuming you’re already taking basic steps to mitigate overfitting, such as using dropout). As long as you aren’t overfitting too badly, you’re likely under capacity."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Trainign and Evaluating a drop-out regularized, stacked GRU model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras import layers\n",
    "from keras.optimizers import RMSprop\n",
    "\n",
    "model = Sequential()\n",
    "model.add(layers.GRU(32, recurrent_dropout=0.5, return_sequence=True,\n",
    "                    input_shape=(None, float_data.shape[-1])))\n",
    "model.add(layers.GRU(64, activation='relu', dropout=0.1, \n",
    "                     recurrent_dropout=0.5))\n",
    "model.compile(optimizer=RMSprop(), loss='mae')\n",
    "history = model.fit_generator(train_gen, steps_per_epoch=500,\n",
    "                             epochs=40, validation_data=val_gen, \n",
    "                             validation_steps=val_steps)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Figure 6.23 shows the results. You can see that the added layer does improve the results a bit, though not significantly. You can draw two conclusions:\n",
    "- Because you’re still not overfitting too badly, you could safely increase the size of your layers in a quest for validation-loss improvement. This has a non-negligible computational cost, though.\n",
    "- Adding a layer didn’t help by a significant factor, so you may be seeing diminish- ing returns from increasing network capacity at this point."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_history(history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using Bidirectional RNNS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The last technique introduced in this section is called bidirectional RNNs. A bidirec- tional RNN is a common RNN variant that can offer greater performance than a regu- lar RNN on certain tasks. It’s frequently used in natural-language processing—you could call it the Swiss Army knife of deep learning for natural-language processing. RNNs are notably order dependent, or time dependent: they process the timesteps of their input sequences in order, and shuffling or reversing the timesteps can com- pletely change the representations the RNN extracts from the sequence. This is pre- cisely the reason they perform well on problems where order is meaningful, such as the temperature-forecasting problem. A bidirectional RNN exploits the order sensitiv- ity of RNNs: it consists of using two regular RNNs, such as the GRU and LSTM layers you’re already familiar with, each of which processes the input sequence in one direc- tion (chronologically and antichronologically), and then merging their representa- tions. By processing a sequence both ways, a bidirectional RNN can catch patterns that may be overlooked by a unidirectional RNN.\n",
    "\n",
    "Remarkably, the fact that the RNN layers in this section have processed sequences in chronological order (older timesteps first) may have been an arbitrary decision. At least, it’s a decision we made no attempt to question so far. Could the RNNs have performed well enough if they processed input sequences in antichronological order, for instance (newer timesteps first)? Let’s try this in practice and see what happens. All you need to do is write a variant of the data generator where the input sequences are reverted along the time dimension (replace the last line with yield samples[:, ::-1, :], targets). Training the same one-GRU-layer network that you used in the first experiment in this section, you get the results shown in figure 6.24."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training and Evaluating an LSTM using reversed sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.datasets import imdb\n",
    "from keras.preprocessing import sequence\n",
    "from keras import layers\n",
    "from kears.models import Sequential\n",
    "\n",
    "max_features = 100000\n",
    "maxlen = 500\n",
    "\n",
    "(x_train, y_train), (x_test, y_test) = imdb.load_data(\n",
    "    num_words=max_features)\n",
    "x_train = [x[::-1] for x in x_train]\n",
    "x_test = [x::-1 for x in x_test]\n",
    "\n",
    "x_train = sequence.pad_sequences(x_train, maxlen=maxlen)\n",
    "x_test = sequence.pad_sequneces(x_test, maxlen=maxlen)\n",
    "\n",
    "model = Sequential()\n",
    "model.add(layers.Embedding(max_features, 128))\n",
    "model.add(layers.LSTM(12))\n",
    "model.add(layers.Dense(1, activation='sigmoid'))\n",
    "\n",
    "model.compile(optimizer='rmsprop', \n",
    "              loss='binary_crossentropy',\n",
    "             metric =['acc'])\n",
    "\n",
    "history = model.fit(x_train, y_train, epochs=10,\n",
    "                    batch_size=128,\n",
    "                    validation_split=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You get performance nearly identical to that of the chronological-order LSTM. Remarkably, on such a text dataset, reversed-order processing works just as well as chronological processing, confirming the hypothesis that, although word order does matter in understanding language, which order you use isn’t crucial. Importantly, an RNN trained on reversed sequences will learn different representations than one trained on the original sequences, much as you would have different mental models if time flowed backward in the real world—if you lived a life where you died on your first day and were born on your last day. In machine learning, representations that are dif- ferent yet useful are always worth exploiting, and the more they differ, the better: they offer a new angle from which to look at your data, capturing aspects of the data that were missed by other approaches, and thus they can help boost performance on a task. This is the intuition behind ensembling, a concept we’ll explore in chapter 7."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training and Evaluating a Bidirectional LSTM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To instantiate a bidirectional RNN in Keras, you use the Bidirectional layer, which takes as its first argument a recurrent layer instance. Bidirectional creates a second, separate instance of this recurrent layer and uses one instance for processing the input sequences in chronological order and the other instance for processing the input sequences in reversed order. Let’s try it on the IMDB sentiment-analysis task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model= Sequential()\n",
    "model.add(layers.Embedding(max_features, 32))\n",
    "model.add(layers.Bidirectional(layers.LSTM(32)))\n",
    "model.add(layers.Dense(1, activation='sigmoid'))\n",
    "\n",
    "model.compile(optimizer='rmsprop', loss='binary_crossentropy',\n",
    "             metrics =['acc'])\n",
    "history = model.fit(x_train, y_train, epochs=10, batch_size=128,\n",
    "                   validation_split=0.2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training a Bidirectioanl GRU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras import layers\n",
    "from keras.optimizers import RMSprop\n",
    "\n",
    "model = Sequential()\n",
    "model.add(layers.Bidirectional(\n",
    "            layers.GRU(32, input_shape=(None, float_data.shape[-1])))\n",
    "model.compile(optimizer=RMSprop(), loss='mae')\n",
    "histor = model.fit_generator(train_gen, \n",
    "                            steps_per_epoch=500,\n",
    "                            epochs = 40,\n",
    "                            validation_data = val_gen,\n",
    "                            validation_steps = val_steps)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This performs about as well as the regular GRU layer. It’s easy to understand why: all the predictive capacity must come from the chronological half of the network, because the antichronological half is known to be severely underperforming on this task (again, because the recent past matters much more than the distant past in this case)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
